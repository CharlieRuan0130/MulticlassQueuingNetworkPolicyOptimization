Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Ilyas2020,
abstract = {We study how the behavior of deep policy gradient algorithms reflects the conceptual framework motivating their development. To this end, we propose a fine-grained analysis of state-of-the-art methods based on key elements of this framework: gradient estimation, value prediction, and optimization landscapes. Our results show that the behavior of deep policy gradient algorithms often deviates from what their motivating framework would predict: the surrogate objective does not match the true reward landscape, learned value estimators fail to fit the true value function, and gradient estimates poorly correlate with the "true" gradient. The mismatch between predicted and empirical behavior we uncover highlights our poor understanding of current methods, and indicates the need to move beyond current benchmark-centric evaluation methods.},
author = {Ilyas, Andrew and Engstrom, Logan and Santurkar, Shibani and Tsipras, Dimitris and Janoos, Firdaus and Rudolph, Larry and Adry, Aleksander M.},
booktitle = {ICLR},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ilyas et al. - 2020 - A Closer Look at Deep Policy Gradients.pdf:pdf},
keywords = {optimization,policy gradient,reinforcement learning},
title = {{A Closer Look at Deep Policy Gradients}},
year = {2020}
}
@article{Greensmith2004a,
author = {Greensmith, Evan and Bartlett, Peter L. and Baxter, Jonathan},
doi = {10.5555/1005332.1044710},
journal = {The Journal of Machine Learning Research},
pages = {1471--1530},
title = {{Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning}},
volume = {5},
year = {2004}
}
@article{Williams1998,
author = {Williams, R.J.},
doi = {10.1023/A:1019108819713},
issn = {02570130},
journal = {Queueing Systems},
number = {1/2},
pages = {27--88},
publisher = {Kluwer Academic Publishers},
title = {{Diffusion approximations for open multiclass queueing networks: sufficient conditions involving state space collapse}},
url = {http://link.springer.com/10.1023/A:1019108819713},
volume = {30},
year = {1998}
}
@article{Harrison1998,
abstract = {This paper is concerned with dynamic scheduling in a queueing system that has two independent Poisson input streams, two servers, deterministic service times and linear holding costs. One server can process both classes of incoming jobs, but the other can process only one class, and the service time for the shared job class is different depending on which server is involved. A bound on system performance is developed in terms of a single pooled resource, or super-server, whose capabilities combine those of the original two servers. Thereafter, attention is focused on the heavy traffic regime, where the combined capacity of the two servers is approximately equal to the total input rate. We construct a discrete-review control policy and show that if its parameters are chosen correctly as one approaches the heavy traffic limit, then its cost performance approaches the bound associated with a single pooled resource. Thus the discrete-review policy is proved to be asymptotically optimal in the heavy traffic limit. Although resource pooling in heavy traffic has been observed to occur in other network scheduling problems, there have been very few studies that rigorously proved the pooling phenomenon, or that proved the asymptotic optimality of a specific policy. Our discrete-review policy is obtained by applying a general method, called the BIGSTEP method in an earlier paper, to the parallel-server model.},
author = {Harrison, J Michael},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Harrison - 1998 - HEAVY TRAFFIC ANALYSIS OF A SYSTEM WITH PARALLEL SERVERS ASYMPTOTIC OPTIMALITY OF DISCRETE-REVIEW POLICIES.pdf:pdf},
journal = {The Annals of Applied Probability},
number = {3},
pages = {822--848},
title = {{Heavy traffic analysis of a system with parallel servers: asymptotic optimality of discrete-review policies}},
url = {https://projecteuclid.org/download/pdf{\_}1/euclid.aoap/1028903452},
volume = {8},
year = {1998}
}
@inproceedings{Kim2017,
author = {Kim, Namyong and Shin, Hayong},
booktitle = {2017 Winter Simulation Conference (WSC)},
doi = {10.1109/WSC.2017.8248209},
isbn = {978-1-5386-3428-8},
month = {dec},
pages = {4570--4571},
publisher = {IEEE},
title = {{The application of actor-critic reinforcement learning for fab dispatching scheduling}},
url = {http://ieeexplore.ieee.org/document/8248209/},
year = {2017}
}
@article{Mnih2016,
abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
archivePrefix = {arXiv},
arxivId = {1602.01783},
author = {Mnih, Volodymyr and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
eprint = {1602.01783},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learning.pdf:pdf},
month = {feb},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1602.01783},
year = {2016}
}
@article{Konda2003,
abstract = {In this article, we propose and analyze a class of actor-critic algorithms. These are two-time-scale algorithms in which the critic uses temporal difference learning with a linearly parameterized approximation architecture, and the actor is updated in an approximate gradient direction, based on information provided by the critic. We show that the features for the critic should ideally span a subspace prescribed by the choice of parameterization of the actor. We study actor-critic algorithms for Markov decision processes with Polish state and action spaces. We state and prove two results regarding their convergence.},
author = {Konda, Vijay R. and Tsitsiklis, John N.},
doi = {10.1137/S0363012901385691},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Konda, Tsitsiklis - 2003 - On actor-critic algorithms.pdf:pdf},
issn = {03630129},
journal = {SIAM Journal on Control and Optimization},
keywords = {Actor-critic algorithms,Markov decision processes,Reinforcement learning,Stochastic approximation},
number = {4},
pages = {1143--1166},
title = {{On actor-critic algorithms}},
volume = {42},
year = {2003}
}
@inproceedings{Mao2016,
address = {New York, New York, USA},
author = {Mao, Hongzi and Alizadeh, Mohammad and Menache, Ishai and Kandula, Srikanth},
booktitle = {Proceedings of the 15th ACM Workshop on Hot Topics in Networks  - HotNets '16},
doi = {10.1145/3005745.3005750},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mao et al. - 2016 - Resource Management with Deep Reinforcement Learning.pdf:pdf},
isbn = {9781450346610},
pages = {50--56},
publisher = {ACM Press},
title = {{Resource Management with Deep Reinforcement Learning}},
url = {http://dl.acm.org/citation.cfm?doid=3005745.3005750},
year = {2016}
}
@article{Glynn1994,
abstract = {This paper offers a short introduction to the regenerative method of steady-state simulation output analysis. The paper also contains several new results. In particular, it is shown that regenerative methods necessarily apply to steady-state simulations that are 'well-posed' in a certain precise sense. The paper also describes a bias-reduction algorithm that takes advantage of regenerative structure. {\textcopyright} 1994 Kluwer Academic Publishers.},
author = {Glynn, Peter W.},
doi = {10.1007/BF00994267},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Glynn - 1994 - Some topics in regenerative steady-state simulation.pdf:pdf},
issn = {01678019},
journal = {Acta Applicandae Mathematicae},
keywords = {Harris chains,Mathematics Subject Classifications (1991): 60K05,,Steady-state simulation,initial transient,regenerative processes},
month = {feb},
number = {1-2},
pages = {225--236},
publisher = {Kluwer Academic Publishers},
title = {{Some topics in regenerative steady-state simulation}},
volume = {34},
year = {1994}
}
@inproceedings{Kingma2017,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy},
booktitle = {ICLR},
eprint = {1412.6980},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma, Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:pdf},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2015}
}
@article{Mnih2015,
abstract = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2015 - Human-level control through deep reinforcement learning.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
keywords = {Computer science},
month = {feb},
number = {7540},
pages = {529--533},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
url = {http://www.nature.com/articles/nature14236},
volume = {518},
year = {2015}
}
@article{Bertsimas1994,
author = {Bertsimas, Dimitris and Paschalidis, Ioannis Ch. and Tsitsiklis, John N},
journal = {The Annals of Applied Probability},
number = {1},
pages = {43--75},
title = {{Optimization of Multiclass Queueing Networks: Polyhedral and Nonlinear Characterizations of Achievable Performance}},
url = {http://ieeexplore.ieee.org/document/1094075/},
volume = {4},
year = {1994}
}
@article{Wolfer2019,
abstract = {We address the problem of estimating the mixing time {\$}t{\_}{\{}\backslashmathsf{\{}mix{\}}{\}}{\$} of an arbitrary ergodic finite-state Markov chain from a single trajectory of length {\$}m{\$}. The reversible case was addressed by Hsu et al. [2019], who left the general case as an open problem. In the reversible case, the analysis is greatly facilitated by the fact that the Markov operator is self-adjoint, and Weyl's inequality allows for a dimension-free perturbation analysis of the empirical eigenvalues. As Hsu et al. point out, in the absence of reversibility (which induces asymmetric pair probabilities matrices), the existing perturbation analysis has a worst-case exponential dependence on the number of states {\$}d{\$}. Furthermore, even if an eigenvalue perturbation analysis with better dependence on {\$}d{\$} were available, in the non-reversible case the connection between the spectral gap and the mixing time is not nearly as straightforward as in the reversible case. Our key insight is to estimate the pseudo-spectral gap {\$}\backslashgamma{\_}{\{}\backslashmathsf{\{}ps{\}}{\}}{\$} instead, which allows us to overcome the loss of symmetry and to achieve a polynomial dependence on the minimal stationary probability {\$}\backslashpi{\_}\backslashstar{\$} and {\$}\backslashgamma{\_}{\{}\backslashmathsf{\{}ps{\}}{\}}{\$}. Additionally, in the reversible case, we obtain simultaneous nearly (up to logarithmic factors) minimax rates in {\$}t{\_}{\{}\backslashmathsf{\{}mix{\}}{\}}{\$} and precision {\$}\backslashvarepsilon{\$}, closing a gap in Hsu et al., who treated {\$}\backslashvarepsilon{\$} as constant in the lower bounds. Finally, we construct fully empirical confidence intervals for {\$}\backslashgamma{\_}{\{}\backslashmathsf{\{}ps{\}}{\}}{\$}, which shrink to zero at a rate of roughly {\$}1/\backslashsqrt{\{}m{\}}{\$}, and improve the state of the art in even the reversible case.},
archivePrefix = {arXiv},
arxivId = {1902.01224},
author = {Wolfer, Geoffrey and Kontorovich, Aryeh},
eprint = {1902.01224},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wolfer, Kontorovich - 2019 - Estimating the Mixing Time of Ergodic Markov Chains.pdf:pdf},
month = {feb},
title = {{Estimating the Mixing Time of Ergodic Markov Chains}},
url = {http://arxiv.org/abs/1902.01224},
year = {2019}
}
@inproceedings{Henderson,
author = {Henderson, S.G. and Glynn, P.W.},
booktitle = {WSC'99. 1999 Winter Simulation Conference Proceedings. 'Simulation - A Bridge to the Future' (Cat. No.99CH37038)},
doi = {10.1109/WSC.1999.823097},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Henderson, Glynn - Unknown - Can the regenerative method be applied to discrete-event simulation.pdf:pdf},
isbn = {0-7803-5780-9},
pages = {367--373},
publisher = {IEEE},
title = {{Can the regenerative method be applied to discrete-event simulation?}},
url = {http://ieeexplore.ieee.org/document/823097/},
volume = {1}
}
@book{Dunkin2009,
author = {Dunkin, Ann. and {Association for Computing Machinery}, Emmanuel},
booktitle = {Winter Simulation Conference},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dunkin, Association for Computing Machinery - 2009 - Winter Simulation Conference.pdf:pdf},
isbn = {9781424457717},
pages = {1634--1645},
publisher = {Winter Simulation Conference},
title = {{Winter Simulation Conference.}},
url = {https://dl.acm.org/citation.cfm?id=1995678},
year = {2009}
}
@inproceedings{Glynn1987,
author = {Glynn, Peter W.},
booktitle = {Proceedings of the 1987 Winter Simulation Conference},
pages = {366--375},
title = {{Likelihood Ratio Gradient Estimation: An Overview}},
url = {http://web.stanford.edu/{~}glynn/papers/1987/G87c.html},
year = {1987}
}
@article{Resnick2018,
abstract = {Model-free reinforcement learning (RL) requires a large number of trials to learn a good policy, especially in environments with sparse rewards. We explore a method to improve the sample efficiency when we have access to demonstrations. Our approach, Backplay, uses a single demonstration to construct a curriculum for a given task. Rather than starting each training episode in the environment's fixed initial state, we start the agent near the end of the demonstration and move the starting point backwards during the course of training until we reach the initial state. Our contributions are that we analytically characterize the types of environments where Backplay can improve training speed, demonstrate the effectiveness of Backplay both in large grid worlds and a complex four player zero-sum game (Pommerman), and show that Backplay compares favorably to other competitive methods known to improve sample efficiency. This includes reward shaping, behavioral cloning, and reverse curriculum generation.},
archivePrefix = {arXiv},
arxivId = {1807.06919},
author = {Resnick, Cinjon and Raileanu, Roberta and Kapoor, Sanyam and Peysakhovich, Alexander and Cho, Kyunghyun and Bruna, Joan},
eprint = {1807.06919},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Resnick et al. - 2018 - Backplay Man muss immer umkehren.pdf:pdf},
month = {jul},
title = {{Backplay: "Man muss immer umkehren"}},
url = {http://arxiv.org/abs/1807.06919},
year = {2018}
}
@book{Bertsekas2012a,
abstract = {Fourth edition. v. 1. [no special title] -- v. 2. Approximate dynamic programming.},
address = {Belmont},
author = {Bertsekas, Dimitri P.},
edition = {4th},
isbn = {1886529434},
pages = {712},
publisher = {Athena Scientific},
title = {{Dynamic programming and optimal control V.II}},
year = {2012}
}
@article{Glynn1996,
abstract = {In this paper we consider $\psi$-irreducible Markov processes evolving in discrete or continuous time on a general state space. We develop a Liapounov function criterion that permits one to obtain explicit bounds on the solution to the Poisson equation and, in particular, obtain conditions under which the solution is square integrable. These results are applied to obtain sufficient conditions that guarantee the validity of a functional central limit theorem for the Markov process. As a second consequence of the bounds obtained, a perturbation theory for Markov processes is developed which gives conditions under which both the solution to the Poisson equation and the invariant probability for the process are continuous functions of its transition kernel. The techniques are illustrated with applications to queueing theory and autoregressive processes.},
author = {Glynn, Peter W. and Meyn, Sean P.},
doi = {10.1214/aop/1039639370},
file = {:C$\backslash$:/Users/mg2289/Downloads/Glynn, Meyn - 1996 - A Liapounov bound for solutions of the Poisson equation.pdf:pdf},
journal = {Annals of Probability},
keywords = {Foster's criterion,Functional central limit theorem,Liapounov function,Markov chain,Markov process,Perturbation theory,Poisson equation},
number = {2},
pages = {916--931},
publisher = {Institute of Mathematical Statistics},
title = {{A Liapounov bound for solutions of the Poisson equation}},
volume = {24},
year = {1996}
}
@article{Serfozo1979,
abstract = {A continuous time Markov decision process with uniformly bounded transition rates is shown to be equivalent to a simpler discrete time Markov decision process for both the discounted and average reward criteria on an infinite horizon. This result clarifies some earlier work in this area.},
author = {Serfozo, Richard F.},
doi = {10.1287/opre.27.3.616},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Serfozo - 1979 - Technical Note—An Equivalence Between Continuous and Discrete Time Markov Decision Processes.pdf:pdf},
issn = {0030-364X},
journal = {Operations Research},
number = {3},
pages = {616--620},
publisher = {Institute for Operations Research and the Management Sciences (INFORMS)},
title = {{Technical Note—An Equivalence Between Continuous and Discrete Time Markov Decision Processes}},
volume = {27},
year = {1979}
}
@book{Meyn2009,
address = {Cambridge},
author = {Meyn, Sean and Tweedie, Richard L.},
doi = {10.1017/CBO9780511626630},
edition = {2nd},
isbn = {9780511626630},
pages = {531},
publisher = {Cambridge University Press},
title = {{Markov Chains and Stochastic Stability}},
url = {http://ebooks.cambridge.org/ref/id/CBO9780511626630},
year = {2009}
}
@article{Henderson2002,
abstract = {?Knowledge of either analytical or numerical approximations should enable more efficient simulation estimators to be constructed.? This principle seems intuitively plausible and certainly attractive, yet no completely satisfactory general methodology has been developed to exploit it. The authors present a new approach for obtaining variance reduction in Markov process simulation that is applicable to a vast array of different performance measures. The approach relies on the construction of a martingale that is then used as an internal control variate.},
author = {Henderson, Shane G. and Glynn, Peter W.},
doi = {10.1287/moor.27.2.253.329},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Henderson, Glynn - 2002 - Approximating martingales for variance reduction in Markov process simulation.pdf:pdf},
issn = {0364765X},
journal = {Mathematics of Operations Research},
keywords = {Markov process,Martingale,Simulation,Variance reduction},
number = {2},
pages = {253--271},
publisher = {INFORMS Inst.for Operations Res.and the Management Sciences},
title = {{Approximating martingales for variance reduction in Markov process simulation}},
volume = {27},
year = {2002}
}
@article{Lehnert2018,
abstract = {In Reinforcement Learning, an intelligent agent has to make a sequence of decisions to accomplish a goal. If this sequence is long, then the agent has to plan over a long horizon. While learning the optimal policy and its value function is a well studied problem in Reinforcement Learning, this paper focuses on the structure of the optimal value function and how hard it is to represent the optimal value function. We show that the generalized Rademacher complexity of the hypothesis space of all optimal value functions is dependent on the planning horizon and independent of the state and action space size. Further, we present bounds on the action-gaps of action value functions and show that they can collapse if a long planning horizon is used. The theoretical results are verified empirically on randomly generated MDPs and on a grid-world fruit collection task using deep value function approximation. Our theoretical results highlight a connection between value function approximation and the Options framework and suggest that value functions should be decomposed along bottlenecks of the MDP's transition dynamics.},
author = {Lehnert, Lucas and Laroche, Romain and van Seijen, Harm},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lehnert, Laroche, Seijen - 2018 - On Value Function Representation of Long Horizon Problems.pdf:pdf},
journal = {Thirty-Second AAAI Conference on Artificial Intelligence},
keywords = {Reinforcement Learning},
title = {{On Value Function Representation of Long Horizon Problems}},
url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16388},
year = {2018}
}
@article{Terekhov2014,
abstract = {Within the combinatorial scheduling community, there has been an increasing interest in modelling and solving scheduling problems in dynamic environments. Such problems have also been considered in the field of queueing theory, but very few papers take advantage of developments in both areas, and literature surveys on dynamic scheduling usually make no mention of queueing approaches. In this paper, we provide an overview of queueing-theoretic models and methods that are relevant to scheduling in dynamic settings. This paper provides a context for investigating the integration of queueing theory and scheduling approaches with the goal of more effectively solving scheduling problems arising in dynamic environments.},
author = {Terekhov, Daria and Down, Douglas G. and Beck, J. Christopher},
doi = {10.1016/J.SORMS.2014.09.001},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Terekhov, Down, Beck - 2014 - Queueing-theoretic approaches for dynamic scheduling A survey.pdf:pdf},
issn = {1876-7354},
journal = {Surveys in Operations Research and Management Science},
month = {jul},
number = {2},
pages = {105--129},
publisher = {Elsevier},
title = {{Queueing-theoretic approaches for dynamic scheduling: A survey}},
url = {https://www.sciencedirect.com/science/article/pii/S1876735414000233},
volume = {19},
year = {2014}
}
@article{Komorowski2018,
abstract = {Sepsis is the third leading cause of death worldwide and the main cause of mortality in hospitals1–3, but the best treatment strategy remains uncertain. In particular, evidence suggests that current practices in the administration of intravenous fluids and vasopressors are suboptimal and likely induce harm in a proportion of patients1,4–6. To tackle this sequential decision-making problem, we developed a reinforcement learning agent, the Artificial Intelligence (AI) Clinician, which extracted implicit knowledge from an amount of patient data that exceeds by many-fold the life-time experience of human clinicians and learned optimal treatment by analyzing a myriad of (mostly suboptimal) treatment decisions. We demonstrate that the value of the AI Clinician's selected treatment is on average reliably higher than human clinicians. In a large validation cohort independent of the training data, mortality was lowest in patients for whom clinicians' actual doses matched the AI decisions. Our model provides individualized and clinically interpretable treatment decisions for sepsis that could improve patient outcomes.},
author = {Komorowski, Matthieu and Celi, Leo A. and Badawi, Omar and Gordon, Anthony C. and Faisal, A. Aldo},
doi = {10.1038/s41591-018-0213-5},
issn = {1078-8956},
journal = {Nature Medicine},
keywords = {Biomedical engineering,Computational models,Machine learning,Outcomes research},
month = {nov},
number = {11},
pages = {1716--1720},
publisher = {Nature Publishing Group},
title = {{The Artificial Intelligence Clinician learns optimal treatment strategies for sepsis in intensive care}},
url = {http://www.nature.com/articles/s41591-018-0213-5},
volume = {24},
year = {2018}
}
@inproceedings{Gu2017,
abstract = {Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.},
archivePrefix = {arXiv},
arxivId = {1611.02247},
author = {Gu, Shixiang and Lillicrap, Timothy and Ghahramani, Zoubin and Turner, Richard E. and Levine, Sergey},
booktitle = {c. International Conference on Learning Representations},
eprint = {1611.02247},
month = {nov},
title = {{Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic}},
url = {http://arxiv.org/abs/1611.02247},
year = {2017}
}
@article{Bellemare2013,
abstract = {In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available.},
author = {Bellemare, Marc G. and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
journal = {Journal of Artificial Intelligence Research},
number = {1},
pages = {253--279},
publisher = {AI Access Foundation},
title = {{The arcade learning environment: an evaluation platform for general agents}},
url = {https://dl.acm.org/citation.cfm?id=2566979},
volume = {47},
year = {2013}
}
@article{CongLuong2018,
abstract = {This paper presents a comprehensive literature review on applications of deep reinforcement learning in communications and networking. Modern networks, e.g., Internet of Things (IoT) and Unmanned Aerial Vehicle (UAV) networks, become more decentralized and autonomous. In such networks, network entities need to make decisions locally to maximize the network performance under uncertainty of network environment. Reinforcement learning has been efficiently used to enable the network entities to obtain the optimal policy including, e.g., decisions or actions, given their states when the state and action spaces are small. However, in complex and large-scale networks, the state and action spaces are usually large, and the reinforcement learning may not be able to find the optimal policy in reasonable time. Therefore, deep reinforcement learning, a combination of reinforcement learning with deep learning, has been developed to overcome the shortcomings. In this survey, we first give a tutorial of deep reinforcement learning from fundamental concepts to advanced models. Then, we review deep reinforcement learning approaches proposed to address emerging issues in communications and networking. The issues include dynamic network access, data rate control, wireless caching, data offloading, network security, and connectivity preservation which are all important to next generation networks such as 5G and beyond. Furthermore, we present applications of deep reinforcement learning for traffic routing, resource sharing, and data collection. Finally, we highlight important challenges, open issues, and future research directions of applying deep reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1810.07862v1},
author = {{Cong Luong}, Nguyen and {Thai Hoang}, Dinh and Gong, Shimin and Niyato, Dusit and Wang, Ping and Liang, Ying-Chang and {In Kim}, Dong},
eprint = {1810.07862v1},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cong Luong et al. - 2018 - Applications of Deep Reinforcement Learning in Communications and Networking A Survey.pdf:pdf},
keywords = {Deep reinforcement learning,caching,communications,data collection,data offloading,deep Q-learning,net-working,rate control,security,spectrum access},
title = {{Applications of Deep Reinforcement Learning in Communications and Networking: A Survey}},
url = {https://arxiv.org/pdf/1810.07862.pdf},
year = {2018}
}
@article{Bell2001,
author = {Bell, S. L. and Williams, R. J.},
doi = {10.1214/AOAP/1015345343},
issn = {1050-5164},
journal = {The Annals of Applied Probability},
keywords = {Queueing networks,dynamic control,heavy traffic,resource pooling},
number = {3},
pages = {608--649},
publisher = {Institute of Mathematical Statistics},
title = {{Dynamic scheduling of a system with two parallel servers in heavy traffic with resource pooling: asymptotic optimality of a threshold policy}},
volume = {11},
year = {2001}
}
@article{Achiam2017,
abstract = {For many applications of reinforcement learning it can be more convenient to specify both a reward function and constraints, rather than trying to design behavior through the reward function. For example, systems that physically interact with or around humans should satisfy safety constraints. Recent advances in policy search algorithms (Mnih et al., 2016, Schulman et al., 2015, Lillicrap et al., 2016, Levine et al., 2016) have enabled new capabilities in high-dimensional control, but do not consider the constrained setting. We propose Constrained Policy Optimization (CPO), the first general-purpose policy search algorithm for constrained reinforcement learning with guarantees for near-constraint satisfaction at each iteration. Our method allows us to train neural network policies for high-dimensional control while making guarantees about policy behavior all throughout training. Our guarantees are based on a new theoretical result, which is of independent interest: we prove a bound relating the expected returns of two policies to an average divergence between them. We demonstrate the effectiveness of our approach on simulated robot locomotion tasks where the agent must satisfy constraints motivated by safety.},
archivePrefix = {arXiv},
arxivId = {1705.10528},
author = {Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
eprint = {1705.10528},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Achiam et al. - 2017 - Constrained Policy Optimization.pdf:pdf},
month = {may},
title = {{Constrained Policy Optimization}},
url = {http://arxiv.org/abs/1705.10528},
year = {2017}
}
@inproceedings{Todorov2012,
author = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
booktitle = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2012.6386109},
isbn = {978-1-4673-1736-8},
month = {oct},
pages = {5026--5033},
publisher = {IEEE},
title = {{MuJoCo: A physics engine for model-based control}},
url = {http://ieeexplore.ieee.org/document/6386109/},
year = {2012}
}
@article{Dai1995,
author = {Dai, J. G.},
doi = {10.1214/aoap/1177004828},
issn = {1050-5164},
journal = {The Annals of Applied Probability},
keywords = {Harris positive recurrent,Multiclass queueing networks,fluid approximation,stability},
month = {feb},
number = {1},
pages = {49--77},
publisher = {Institute of Mathematical Statistics},
title = {{On Positive Harris Recurrence of Multiclass Queueing Networks: A Unified Approach Via Fluid Limit Models}},
url = {http://projecteuclid.org/euclid.aoap/1177004828},
volume = {5},
year = {1995}
}
@article{Schweitzer1968,
abstract = {A perturbation formalism is presented which shows how the stationary distribution and fundamental matrix of a Markov chain containing a single irreducible set of states change as the transition probabilities vary. Expressions are given for the partial derivatives of the stationary distribution and fundamental matrix with respect to the transition probabilities. Semi-group properties of the generators of transformations from one Markov chain to another are investigated. It is shown that a perturbation formalism exists in the multiple subchain case if and only if the change in the transition probabilities does not alter the number of, or intermix the various subchains. The formalism is presented when this condition is satisfied.},
author = {Schweitzer, Paul J.},
doi = {10.2307/3212261},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schweitzer - 1968 - Perturbation theory and finite Markov chains.pdf:pdf},
issn = {0021-9002},
journal = {Journal of Applied Probability},
number = {2},
pages = {401--413},
title = {{Perturbation theory and finite Markov chains}},
url = {https://www.cambridge.org/core/product/identifier/S0021900200110083/type/journal{\_}article},
volume = {5},
year = {1968}
}
@article{Bartlett2002,
abstract = {We model reinforcement learning as the problem of learning to control a partially observable Markov decision process (POMDP) and focus on gradient ascent approaches to this problem. In an earlier work (2001, J. Artificial Intelligence Res. 14) we introduced GPOMDP, an algorithm for estimating the performance gradient of a POMDP from a single sample path, and we proved that this algorithm almost surely converges to an approximation to the gradient. In this paper, we provide a convergence rate for the estimates produced by GPOMDP and give an improved bound on the approximation error of these estimates. Both of these bounds are in terms of mixing times of the POMDP. {\textcopyright} 2002 Elsevier Science (USA).},
author = {Bartlett, Peter L. and Baxter, Jonathan},
doi = {10.1006/jcss.2001.1793},
issn = {00220000},
journal = {Journal of Computer and System Sciences},
number = {1},
pages = {133--150},
publisher = {Academic Press Inc.},
title = {{Estimation and approximation bounds for gradient-based reinforcement learning}},
volume = {64},
year = {2002}
}
@article{Jiang2017,
abstract = {In this paper, we are devoted to singular perturbation analysis for discrete-time or continuous-time Markov chains. We modify and extend the drift condition method, well known for regular perturbation, to develop a new framework for singular perturbation analysis. Our results extend and improve the corresponding ones in [2] for singularly perturbed Markov chains by allowing a general perturbation form, less restrictive conditions, and more computable bounds. Our analysis covers the regular perturbation analysis, and hence unifies singular and regular perturbation analysis. Furthermore, our results are illustrated by two two-dimensional Markov chains, including a discrete-time queue and a continuous-time level dependent quasi-birth-death process.},
author = {Jiang, Shuxia and Liu, Yuanyuan and Tang, Yingchun},
doi = {10.1016/j.laa.2017.05.002},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang, Liu, Tang - 2017 - A unified perturbation analysis framework for countable Markov chains.pdf:pdf},
issn = {00243795},
journal = {Linear Algebra and Its Applications},
keywords = {Markov chains,Perturbation analysis,Queues,Stationary distribution},
pages = {413--440},
publisher = {Elsevier Inc.},
title = {{A unified perturbation analysis framework for countable Markov chains}},
volume = {529},
year = {2017}
}
@article{Kartashov1985,
abstract = {In this paper, we are interested in investigating the perturbation bounds for the stationary distributions for discrete-time or continuous-time Markov chains on a countable state space. For discrete-time Markov chains, two new norm-wise bounds are obtained. The first bound is rather easy to be obtained since the needed condition, equivalent to uniform ergodicity, is imposed on the transition matrix directly. The second bound, which holds for a general (possibly periodic) Markov chain, involves finding a drift function. This drift function is closely related with the mean first hitting times. Some V-norm-wise bounds are also derived based on the results in [11]. Moreover, we show how the bounds developed in this paper and one bound given in [24] can be extended to continuous-time Markov chains. Several examples are shown to illustrate our results or to compare our bounds with the known ones in the literature.},
archivePrefix = {arXiv},
arxivId = {1208.4974v1},
author = {Kartashov, N.},
eprint = {1208.4974v1},
journal = {Theory Probab. Appl.},
pages = {71--89},
title = {{Criteria for uniform ergodicity and strong stability of Markov chains with a common phase space}},
url = {https://arxiv.org/pdf/1208.4974.pdf},
volume = {30},
year = {1985}
}
@article{Bhatnagar2012,
abstract = {We develop an online actor-critic reinforcement learning algorithm with function approximation for a problem of control under inequality constraints. We consider the long-run average cost Markov decision process (MDP) framework in which both the objective and the constraint functions are suitable policy-dependent long-run averages of certain sample path functions. The Lagrange multiplier method is used to handle the inequality constraints. We prove the asymptotic almost sure convergence of our algorithm to a locally optimal solution. We also provide the results of numerical experiments on a problem of routing in a multi-stage queueing network with constraints on long-run average queue lengths. We observe that our algorithm exhibits good performance on this setting and converges to a feasible point. {\textcopyright} 2012 Springer Science+Business Media, LLC.},
author = {Bhatnagar, Shalabh and Lakshmanan, K.},
doi = {10.1007/s10957-012-9989-5},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bhatnagar, Lakshmanan - 2012 - An Online Actor-Critic Algorithm with Function Approximation for Constrained Markov Decision Processes.pdf:pdf},
issn = {00223239},
journal = {Journal of Optimization Theory and Applications},
keywords = {Actor-critic algorithm,Constrained Markov decision processes,Function approximation,Long-run average cost criterion},
number = {3},
pages = {688--708},
publisher = {Springer},
title = {{An Online Actor-Critic Algorithm with Function Approximation for Constrained Markov Decision Processes}},
volume = {153},
year = {2012}
}
@article{Cooper2003,
abstract = {Simulation-based policy iteration (SBPI) is a modification of the policy iteration algorithm for computing optimal policies for Markov decision processes. At each iteration, rather than solving the average evaluation equations, SBPI employs simulation to estimate a solution to these equations. For recurrent average-reward Markov decision processes with finite state and action spaces, we provide easily verifiable conditions that ensure that simulation-based policy iteration almostsurely eventually never leaves the set of optimal decision rules. We analyze three simulation estimators for solutions to the average evaluation equations. Using our general results, we derive simple conditions on the simulation run lengths that guarantee the almost-sure convergence of the algorithm.},
author = {Cooper, William L. and Henderson, Shane G. and Lewis, Mark E.},
doi = {10.1017/S0269964803172051},
issn = {02699648},
journal = {Probability in the Engineering and Informational Sciences},
number = {2},
pages = {213--234},
title = {{Convergence of simulation-based policy iteration}},
volume = {17},
year = {2003}
}
@article{Nelson1989,
abstract = {This paper considers the combined use of control variates and batching for estimating the steady-state mean of an infinite-horizon process via simulation. Properties of the point and interval estimators from such a procedure are derived as functions of the number of batches and the number of control variates when the total sample size is fixed. {\textcopyright} 1989.},
author = {Nelson, Barry L.},
doi = {10.1016/0377-2217(89)90212-9},
issn = {03772217},
journal = {European Journal of Operational Research},
keywords = {Simulation,multivariate statistics,regression},
number = {2},
pages = {184--196},
title = {{Batch size effects on the efficiency of control variates in simulation}},
volume = {43},
year = {1989}
}
@incollection{Avram1995,
address = {New York},
author = {Avram, Florin and Bertsimas, Dimitris and Ricard, Michael},
booktitle = {Stochastic Networks},
editor = {Kelly, F. and Williams, R. J.},
pages = {237},
publisher = {Springer},
title = {{Fluid models of sequencing problems in open queueing networks: an optimal control approach}},
volume = {71},
year = {1995}
}
@article{Andradottir1993,
abstract = {We consider the simulation of a discrete Markov chain that is so large that numerical solution of the steady-state balance equations cannot be done with available computers, We propose smoothing methods to obtain variance reduction when simulation is used to estimate a function of a subset of the steady-state probabilities. These methods attempt to make each transition provide information about the probabilities of interest. We give an algorithm that converges to the optimal smoothing operator, and some guidelines for picking the parameters of this algorithm. Analytical arguments are used to justify our procedures, and they are buttressed by the results of a numerical example. {\textcopyright} 1993, ACM. All rights reserved.},
author = {Andrad{\'{o}}ttir, Sigr{\'{u}}n and Heyman, Daniel P. and Ott, Teunis J.},
doi = {10.1145/174153.174154},
issn = {15581195},
journal = {ACM Transactions on Modeling and Computer Simulation (TOMACS)},
keywords = {Poisson's equation,conditional Monte Carlo},
number = {3},
pages = {167--189},
title = {{Variance Reduction Through Smoothing and Control Variates for Markov Chain Simulations}},
volume = {3},
year = {1993}
}
@book{Meyn2007,
abstract = {Power grids, flexible manufacturing, cellular communications: interconnectedness has consequences. This remarkable book gives the tools and philosophy you need to build network models detailed enough to capture essential dynamics but simple enough to expose the structure of effective control solutions and to clarify analysis. Core chapters assume only exposure to stochastic processes and linear algebra at the undergraduate level; later chapters are for advanced graduate students and researchers/practitioners. This gradual development bridges classical theory with the state-of-the-art. The workload model that is the basis of traditional analysis of the single queue becomes a foundation for workload relaxations used in the treatment of complex networks. Lyapunov functions and dynamic programming equations lead to the celebrated MaxWeight policy along with many generalizations. Other topics include methods for synthesizing hedging and safety stocks, stability theory for networks, and techniques for accelerated simulation. Examples and figures throughout make ideas concrete. Solutions to end-of-chapter exercises available on a companion website.},
author = {Meyn, Sean},
booktitle = {Control Techniques for Complex Networks},
doi = {10.1017/CBO9780511804410},
isbn = {9780511804410},
month = {jan},
pages = {562},
publisher = {Cambridge University Press},
title = {{Control techniques for complex networks}},
year = {2007}
}
@article{Wilson2014,
author = {Wilson, Aaron and Fern, Alan and Tadepalli, Prasad},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wilson, Fern, Tadepalli - 2014 - Using Trajectory Data to Improve Bayesian Optimization for Reinforcement Learning.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {Wilson2014},
pages = {253--282},
title = {{Using Trajectory Data to Improve Bayesian Optimization for Reinforcement Learning}},
url = {http://jmlr.org/papers/v15/wilson14a.html},
volume = {15},
year = {2014}
}
@techreport{Hosu,
abstract = {This paper introduces a novel method for learning how to play the most difficult Atari 2600 games from the Arcade Learning Environment using deep reinforcement learning. The proposed method, called human checkpoint replay, consists in using checkpoints sampled from human gameplay as starting points for the learning process. This is meant to compensate for the difficulties of current exploration strategies, such as $\epsilon$-greedy, to find successful control policies in games with sparse rewards. Like other deep reinforcement learning architectures, our model uses a convolutional neural network that receives only raw pixel inputs to estimate the state value function. We tested our method on Montezuma's Revenge and Private Eye, two of the most challenging games from the Atari platform. The results we obtained show a substantial improvement compared to previous learning approaches, as well as over a random player. We also propose a method for training deep reinforcement learning agents using human gameplay experience, which we call human experience replay.},
archivePrefix = {arXiv},
arxivId = {1607.05077v1},
author = {Hosu, Ionel-Alexandru and Rebedea, Traian},
eprint = {1607.05077v1},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hosu, Rebedea - 2016 - Playing Atari Games with Deep Reinforcement Learning and Human Checkpoint Replay.pdf:pdf},
title = {{Playing Atari Games with Deep Reinforcement Learning and Human Checkpoint Replay}},
url = {https://arxiv.org/pdf/1607.05077.pdf},
year = {2016}
}
@article{Bhonker2016,
abstract = {Mastering a video game requires skill, tactics and strategy. While these attributes may be acquired naturally by human players, teaching them to a computer program is a far more challenging task. In recent years, extensive research was carried out in the field of reinforcement learning and numerous algorithms were introduced, aiming to learn how to perform human tasks such as playing video games. As a result, the Arcade Learning Environment (ALE) (Bellemare et al., 2013) has become a commonly used benchmark environment allowing algorithms to train on various Atari 2600 games. In many games the state-of-the-art algorithms outperform humans. In this paper we introduce a new learning environment, the Retro Learning Environment --- RLE, that can run games on the Super Nintendo Entertainment System (SNES), Sega Genesis and several other gaming consoles. The environment is expandable, allowing for more video games and consoles to be easily added to the environment, while maintaining the same interface as ALE. Moreover, RLE is compatible with Python and Torch. SNES games pose a significant challenge to current algorithms due to their higher level of complexity and versatility.},
archivePrefix = {arXiv},
arxivId = {1611.02205},
author = {Bhonker, Nadav and Rozenberg, Shai and Hubara, Itay},
eprint = {1611.02205},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bhonker, Rozenberg, Hubara - 2016 - Playing SNES in the Retro Learning Environment.pdf:pdf;:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bhonker, Rozenberg, Hubara - 2016 - Playing SNES in the Retro Learning Environment(2).pdf:pdf},
month = {nov},
title = {{Playing SNES in the Retro Learning Environment}},
url = {http://arxiv.org/abs/1611.02205},
year = {2016}
}
@inproceedings{Abbasi_Yadkori2014a,
author = {Abbasi-Yadkori, Yasin and Bartlett, Peter and Malek, Alan},
booktitle = {Proceeding ICML'14 Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
pages = {496--504},
title = {{Linear programming for large-scale Markov decision problems}},
url = {https://dl.acm.org/citation.cfm?id=3044948},
year = {2014}
}
@article{Cao1999,
abstract = {Motivated by the needs of on-line optimization of real-world engineering systems, we studied single sample path-based algorithms for Markov decision problems (MDP). The sample path used in the algorithms can be obtained by observing the operation of a real system. We give a simple example to explain the advantages of the sample path-based approach over the traditional computation-based approach: matrix inversion is not required; some transition probabilities do not have to be known; it may save storage space; and it gives the flexibility of iterating the actions for a subset of the state space in each iteration. The effect of the estimation errors and the convergence property of the sample path-based approach are studied. Finally, we propose a fast algorithm, which updates the policy whenever the system reaches a particular set of states and prove that the algorithm converges to the true optimal policy with probability one under some conditions. The sample path-based approach may have important applications to the design and management of engineering systems, such as high speed communication networks.},
author = {Cao, X R},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cao - 1999 - Single Sample Path-Based Optimization of Markov Chains.pdf:pdf},
journal = {JOURNAL OF OPTIMIZATION THEORY AND APPLICATIONS},
keywords = {Markov deci-sion processes,Perturbation analysis,on-line optimization,performance potentials},
number = {3},
pages = {527--548},
title = {{Single Sample Path-Based Optimization of Markov Chains}},
volume = {100},
year = {1999}
}
@inproceedings{Kennedy1995,
author = {Kennedy, J. and Eberhart, R.},
booktitle = {Proceedings of ICNN'95 - International Conference on Neural Networks},
doi = {10.1109/ICNN.1995.488968},
isbn = {0-7803-2768-3},
pages = {1942--1948},
publisher = {IEEE},
title = {{Particle swarm optimization}},
url = {http://ieeexplore.ieee.org/document/488968/},
volume = {4},
year = {1995}
}
@article{Beutler1987,
abstract = {Uniformization permits the replacement of a semi-Markov decision process (SMDP) by a Markov chain exhibiting the same average rewards for simple (non-randomized) policies. It is shown that various anomalies may occur, especially for stationary (randomized) policies; uniformization introduces virtual jumps with concomitant action changes not present in the original process. Since these lead to discrepancies in the average rewards for stationary processes, uniformization can be accepted as valid only for simple policies.},
author = {Beutler, Frederick J. and Ross, Keith W.},
doi = {10.2307/3214096},
issn = {0021-9002},
journal = {Journal of Applied Probability},
number = {3},
pages = {644--656},
publisher = {Applied Probability Trust},
title = {{Uniformization for semi-Markov decision processes under stationary policies}},
url = {https://www.cambridge.org/core/product/identifier/S0021900200031375/type/journal{\_}article},
volume = {24},
year = {1987}
}
@book{Bertsekas2013,
address = {Belmont, MA},
archivePrefix = {arXiv},
arxivId = {1608.01670},
author = {Bertsekas, Dimitri},
eprint = {1608.01670},
publisher = {Athena Scientific},
title = {{Abstract Dynamic Programming}},
url = {http://arxiv.org/abs/1608.01670},
year = {2013}
}
@article{Brochu2010,
abstract = {We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.},
archivePrefix = {arXiv},
arxivId = {1012.2599},
author = {Brochu, Eric and Cora, Vlad M. and de Freitas, Nando},
eprint = {1012.2599},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brochu, Cora, de Freitas - 2010 - A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Model.pdf:pdf},
month = {dec},
title = {{A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning}},
url = {http://arxiv.org/abs/1012.2599},
year = {2010}
}
@article{Sidford2017,
abstract = {In this paper we provide faster algorithms for approximately solving discounted Markov Decision Processes in multiple parameter regimes. Given a discounted Markov Decision Process (DMDP) with {\$}|S|{\$} states, {\$}|A|{\$} actions, discount factor {\$}\backslashgamma\backslashin(0,1){\$}, and rewards in the range {\$}[-M, M]{\$}, we show how to compute an {\$}\backslashepsilon{\$}-optimal policy, with probability {\$}1 - \backslashdelta{\$} in time $\backslash$[ $\backslash$tilde{\{}O{\}}$\backslash$left( $\backslash$left(|S|{\^{}}2 |A| + $\backslash$frac{\{}|S| |A|{\}}{\{}(1 - $\backslash$gamma){\^{}}3{\}} $\backslash$right) $\backslash$log$\backslash$left( $\backslash$frac{\{}M{\}}{\{}$\backslash$epsilon{\}} $\backslash$right) $\backslash$log$\backslash$left( $\backslash$frac{\{}1{\}}{\{}$\backslash$delta{\}} $\backslash$right) $\backslash$right) {\~{}} . $\backslash$] This contribution reflects the first nearly linear time, nearly linearly convergent algorithm for solving DMDPs for intermediate values of {\$}\backslashgamma{\$}. We also show how to obtain improved sublinear time algorithms provided we can sample from the transition function in {\$}O(1){\$} time. Under this assumption we provide an algorithm which computes an {\$}\backslashepsilon{\$}-optimal policy with probability {\$}1 - \backslashdelta{\$} in time $\backslash$[ $\backslash$tilde{\{}O{\}} $\backslash$left($\backslash$frac{\{}|S| |A| M{\^{}}2{\}}{\{}(1 - $\backslash$gamma){\^{}}4 $\backslash$epsilon{\^{}}2{\}} $\backslash$log $\backslash$left($\backslash$frac{\{}1{\}}{\{}$\backslash$delta{\}}$\backslash$right) $\backslash$right) {\~{}}. $\backslash$] Lastly, we extend both these algorithms to solve finite horizon MDPs. Our algorithms improve upon the previous best for approximately computing optimal policies for fixed-horizon MDPs in multiple parameter regimes. Interestingly, we obtain our results by a careful modification of approximate value iteration. We show how to combine classic approximate value iteration analysis with new techniques in variance reduction. Our fastest algorithms leverage further insights to ensure that our algorithms make monotonic progress towards the optimal value. This paper is one of few instances in using sampling to obtain a linearly convergent linear programming algorithm and we hope that the analysis may be useful more broadly.},
archivePrefix = {arXiv},
arxivId = {1710.09988},
author = {Sidford, Aaron and Wang, Mengdi and Wu, Xian and Ye, Yinyu},
eprint = {1710.09988},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sidford et al. - 2017 - Variance Reduced Value Iteration and Faster Algorithms for Solving Markov Decision Processes.pdf:pdf},
month = {oct},
title = {{Variance Reduced Value Iteration and Faster Algorithms for Solving Markov Decision Processes}},
url = {http://arxiv.org/abs/1710.09988},
year = {2017}
}
@article{Arapostathis2019,
author = {Arapostathis, Ari},
doi = {10.1287/stsy.2019.0040},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arapostathis - 2019 - Open Problem—Convergence and Asymptotic Optimality of the Relative Value Iteration in Ergodic Control.pdf:pdf},
issn = {1946-5238},
journal = {Stochastic Systems},
month = {sep},
publisher = {Institute for Operations Research and the Management Sciences (INFORMS)},
title = {{Open Problem—Convergence and Asymptotic Optimality of the Relative Value Iteration in Ergodic Control}},
year = {2019}
}
@incollection{Meyn1997,
address = {Providence, RI},
author = {Meyn, S.P.},
booktitle = {Mathematics of Stochastic Manufacturing Systems},
doi = {10.1239/jap/1421763321},
editor = {Yin, G. George and Zhang, Qing},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Meyn - 1997 - Stability and optimization of queueing networks and their fluid models.pdf:pdf},
pages = {175--199},
publisher = {American Mathematical Society},
title = {{Stability and optimization of queueing networks and their fluid models}},
year = {1997}
}
@article{Liu2012,
abstract = {In this paper, we are interested in investigating the perturbation bounds for the stationary distributions for discrete-time or continuous-time Markov chains on a countable state space. For discrete-time Markov chains, two new normwise bounds are obtained. The first bound is rather easy to obtain since the needed condition, equivalent to uniform ergodicity, is imposed on the transition matrix directly. The second bound, which holds for a general (possibly periodic) Markov chain, involves finding a drift function. This drift function is closely related to the mean first hitting times. Some {\$}V{\$}-normwise bounds are also derived based on the results in [N. V. Kartashov, J. Soviet Math., 34 (1986), pp. 1493--1498]. Moreover, we show how the bounds developed in this paper and one bound given in [E. Seneta, Adv. Appl. Probab., 20 (1988), pp. 228--230] can be extended to continuous-time Markov chains. Several examples are shown to illustrate our results or to compare our bounds with the known ones in the literature.},
author = {Liu, Yuanyuan},
doi = {10.1137/110838753},
issn = {0895-4798},
journal = {SIAM Journal on Matrix Analysis and Applications},
keywords = {15B51,60J10,60J27,Markov chains,mean first hitting times,perturbation theory,stationary distribution,uniform ergodicity},
month = {jan},
number = {4},
pages = {1057--1074},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Perturbation Bounds for the Stationary Distributions of Markov Chains}},
url = {http://epubs.siam.org/doi/10.1137/110838753},
volume = {33},
year = {2012}
}
@article{Williams1992,
abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
author = {Williams, Ronald J.},
doi = {10.1007/bf00992696},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Williams - 1992 - Simple statistical gradient-following algorithms for connectionist reinforcement learning.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
month = {may},
number = {3-4},
pages = {229--256},
publisher = {Springer Science and Business Media LLC},
title = {{Simple statistical gradient-following algorithms for connectionist reinforcement learning}},
volume = {8},
year = {1992}
}
@techreport{Arapostathis2019a,
abstract = {We study the well-posedness of the Bellman equation for the ergodic control problem for a controlled Markov process in R d for a near-monotone cost and establish convergence results for the associated 'relative value iteration' algorithm which computes its solution recursively. In addition, we present some results concerning the stability and asymptotic optimality of the associated rolling horizon policies.},
archivePrefix = {arXiv},
arxivId = {1902.01048v1},
author = {Arapostathis, Ari and Borkar, Vivek S},
eprint = {1902.01048v1},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arapostathis, Borkar - 2019 - AVERAGE COST OPTIMAL CONTROL UNDER WEAK HYPOTHESES RELATIVE VALUE ITERATIONS.pdf:pdf},
title = {{AVERAGE COST OPTIMAL CONTROL UNDER WEAK HYPOTHESES: RELATIVE VALUE ITERATIONS}},
year = {2019}
}
@article{Kumar2001,
author = {Kumar, S. and Kumar, P.R.},
doi = {10.1109/70.964657},
issn = {1042296X},
journal = {IEEE Transactions on Robotics and Automation},
number = {5},
pages = {548--561},
title = {{Queueing network models in the design and analysis of semiconductor wafer fabs}},
url = {http://ieeexplore.ieee.org/document/964657/},
volume = {17},
year = {2001}
}
@inproceedings{Ramirez-Hernandez2010,
author = {Ramirez-Hernandez, Jose A. and Fernandez, Emmanuel},
booktitle = {49th IEEE Conference on Decision and Control (CDC)},
doi = {10.1109/CDC.2010.5717523},
isbn = {978-1-4244-7745-6},
month = {dec},
pages = {3944--3949},
publisher = {IEEE},
title = {{Optimization of Preventive Maintenance scheduling in semiconductor manufacturing models using a simulation-based Approximate Dynamic Programming approach}},
url = {http://ieeexplore.ieee.org/document/5717523/},
year = {2010}
}
@article{Moran1947,
author = {Moran, P. A. P.},
doi = {10.2307/2983572},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Moran - 1947 - The Random Division of an Interval.pdf:pdf},
issn = {14666162},
journal = {Supplement to the Journal of the Royal Statistical Society},
number = {1},
pages = {92},
title = {{The Random Division of an Interval}},
url = {https://www.jstor.org/stable/2983572?origin=crossref},
volume = {9},
year = {1947}
}
@incollection{Harrison1988a,
abstract = {Consider an open queueing network with I single-server stations and K customer classes. Each customer class requires service at a specified station, and customers change class after service in a Markovian fashion. (With K allowed to be arbitrary, this routing structure is almost perfectly general.) There is a renewal input process and general service time distribution for each class. The correspondence between customer classes and service stations is in general many to one, and the service discipline (or scheduling protocol) at each station is left as a matter for dynamic decision making.},
author = {Harrison, J. Michael},
doi = {10.1007/978-1-4613-8762-6_11},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Harrison - 1988 - Brownian Models of Queueing Networks with Heterogeneous Customer Populations.pdf:pdf},
pages = {147--186},
publisher = {Springer, New York, NY},
title = {{Brownian Models of Queueing Networks with Heterogeneous Customer Populations}},
year = {1988}
}
@inproceedings{Ramirez-Hernandez2007c,
author = {Ramirez-Hernandez, Jose A. and Fernandez, Emmanuel},
booktitle = {Sixth International Conference on Machine Learning and Applications (ICMLA 2007)},
doi = {10.1109/ICMLA.2007.78},
isbn = {978-0-7695-3069-7},
month = {dec},
pages = {330--335},
publisher = {IEEE},
title = {{Control of a re-entrant line manufacturing model with a reinforcement learning approach}},
url = {http://ieeexplore.ieee.org/document/4457252/},
year = {2007}
}
@inproceedings{Duan2016,
author = {Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning},
pages = {1329--1338},
publisher = {JMLR.org},
title = {{Benchmarking deep reinforcement learning for continuous control}},
url = {https://dl.acm.org/citation.cfm?id=3045531},
year = {2016}
}
@article{Ouelhadj2009,
author = {Ouelhadj, Djamila and Petrovic, Sanja},
doi = {10.1007/s10951-008-0090-8},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ouelhadj, Petrovic - 2009 - A survey of dynamic scheduling in manufacturing systems.pdf:pdf},
issn = {1094-6136},
journal = {Journal of Scheduling},
month = {aug},
number = {4},
pages = {417--431},
publisher = {Springer US},
title = {{A survey of dynamic scheduling in manufacturing systems}},
url = {http://link.springer.com/10.1007/s10951-008-0090-8},
volume = {12},
year = {2009}
}
@article{Bertsimas2017,
abstract = {Dynamic resource allocation (DRA) problems constitute an important class of dynamic stochastic optimization problems that arise in many real-world applications. DRA problems are notoriously difficult to solve since they combine stochastic dynamics with intractably large state and action spaces. Although the artificial intelligence and operations research communities have independently proposed two successful frameworks for solving such problems—Monte Carlo tree search (MCTS) and rolling horizon optimization (RHO), respectively—the relative merits of these two approaches are not well understood. In this paper, we adapt MCTS and RHO to two problems – a problem inspired by tactical wildfire management and a classical problem involving the control of queueing networks – and undertake an extensive computational study comparing the two methods on large scale instances of both problems in terms of both the state and the action spaces. Both methods are able to greatly improve on a baseline, problem-specific heuristic. On smaller instances, the MCTS and RHO approaches perform comparably, but RHO outperforms MCTS as the size of the problem increases for a fixed computational budget.},
author = {Bertsimas, Dimitris and Griffith, J. Daniel and Gupta, Vishal and Kochenderfer, Mykel J. and Mi{\v{s}}i{\'{c}}, Velibor V.},
doi = {10.1016/J.EJOR.2017.05.032},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bertsimas et al. - 2017 - A comparison of Monte Carlo tree search and rolling horizon optimization for large-scale dynamic resource allo.pdf:pdf},
issn = {0377-2217},
journal = {European Journal of Operational Research},
month = {dec},
number = {2},
pages = {664--678},
publisher = {North-Holland},
title = {{A comparison of Monte Carlo tree search and rolling horizon optimization for large-scale dynamic resource allocation problems}},
url = {https://www.sciencedirect.com/science/article/pii/S0377221717304605},
volume = {263},
year = {2017}
}
@inproceedings{Sutton1999,
abstract = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy. Large applications of reinforcement learning (RL) require the use of generalizing function approximators such neural networks, decision-trees, or instance-based methods. The dominant approach for the last decade has been the value-function approach, in which all function approximation effort goes into estimating a value function, with the action-selection policy represented implicitly as the "greedy" policy with respect to the estimated values (e.g., as the policy that selects in each state the action with highest estimated value). The value-function approach has worked well in many applications , but has several limitations. First, it is oriented toward finding deterministic policies, whereas the optimal policy is often stochastic, selecting different actions with specific probabilities (e.g., see Singh, Jaakkola, and Jordan, 1994). Second, an arbitrarily small change in the estimated value of an action can cause it to be, or not be, selected. Such discontinuous changes have been identified as a key obstacle to establishing convergence assurances for algorithms following the value-function approach (Bertsekas and Tsitsiklis, 1996). For example, Q-Iearning, Sarsa, and dynamic programming methods have all been shown unable to converge to any policy for simple MDPs and simple function approximators (Gordon, 1995, 1996; Baird, 1995; Tsit-siklis and van Roy, 1996; Bertsekas and Tsitsiklis, 1996). This can occur even if the best approximation is found at each step before changing the policy, and whether the notion of "best" is in the mean-squared-error sense or the slightly different senses of residual-gradient, temporal-difference, and dynamic-programming methods. In this paper we explore an alternative approach to function approximation in RL.},
author = {Sutton, Richard S and Mcallester, David and Singh, Satinder and Mansour, Yishay},
booktitle = {NIPS},
pages = {1057--1063},
title = {{Policy Gradient Methods for Reinforcement Learning with Function Approximation}},
year = {1999}
}
@book{Dai2019,
author = {Dai, J.G. and Harrison, J. Michael},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dai, Harrison - 2019 - Processing Networks Fluid Models and Stability.pdf:pdf},
title = {{Processing Networks: Fluid Models and Stability}},
url = {https://spnbook.org/},
year = {2019}
}
@techreport{Henderson2005,
abstract = {We use simulation to estimate the steady-state performance of a stable multiclass queueing network. Standard estimators have been seen to perform poorly when the network is heavily loaded. We introduce two new simulation estimators. The first provides substantial variance reductions in moderately-loaded networks at very little additional computational cost. The second estimator provides substantial variance reductions in heavy traffic, again for a small additional computational cost. Both methods employ the variance reduction method of control variates, and differ in terms of how the control variates are constructed.},
author = {Henderson, Shane G and Meyn, Sean P},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Henderson, Meyn - 2005 - Variance Reduction in Simulation of Multiclass Processing Networks.pdf:pdf},
title = {{Variance Reduction in Simulation of Multiclass Processing Networks}},
year = {2005}
}
@book{Sutton2018,
abstract = {Second edition. "Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."-- Machine generated contents note: 1.Introduction -- 1.1.Reinforcement Learning -- 1.2.Examples -- 1.3.Elements of Reinforcement Learning -- 1.4.Limitations and Scope -- 1.5.An Extended Example: Tic-Tac-Toe -- 1.6.Summary -- 1.7.Early History of Reinforcement Learning -- 2.Multi-armed Bandits -- 2.1.A k-armed Bandit Problem -- 2.2.Action-value Methods -- 2.3.The 10-armed Testbed -- 2.4.Incremental Implementation -- 2.5.Tracking a Nonstationary Problem -- 2.6.Optimistic Initial Values -- 2.7.Upper-Confidence-Bound Action Selection -- 2.8.Gradient Bandit Algorithms -- 2.9.Associative Search (Contextual Bandits) -- 2.10.Summary -- 3.Finite Markov Decision Processes -- 3.1.The Agent-Environment Interface -- 3.2.Goals and Rewards -- 3.3.Returns and Episodes -- 3.4.Unified Notation for Episodic and Continuing Tasks -- 3.5.Policies and Value Functions -- 3.6.Optimal Policies and Optimal Value Functions -- 3.7.Optimality and Approximation -- 3.8.Summary -- 4.Dynamic Programming Note continued: 4.1.Policy Evaluation (Prediction) -- 4.2.Policy Improvement -- 4.3.Policy Iteration -- 4.4.Value Iteration -- 4.5.Asynchronous Dynamic Programming -- 4.6.Generalized Policy Iteration -- 4.7.Efficiency of Dynamic Programming -- 4.8.Summary -- 5.Monte Carlo Methods -- 5.1.Monte Carlo Prediction -- 5.2.Monte Carlo Estimation of Action Values -- 5.3.Monte Carlo Control -- 5.4.Monte Carlo Control without Exploring Starts -- 5.5.Off-policy Prediction via Importance Sampling -- 5.6.Incremental Implementation -- 5.7.Off-policy Monte Carlo Control -- 5.8.*Discounting-aware Importance Sampling -- 5.9.*Per-decision Importance Sampling -- 5.10.Summary -- 6.Temporal-Difference Learning -- 6.1.TD Prediction -- 6.2.Advantages of TD Prediction Methods -- 6.3.Optimality of TD(0) -- 6.4.Sarsa: On-policy TD Control -- 6.5.Q-learning: Off-policy TD Control -- 6.6.Expected Sarsa -- 6.7.Maximization Bias and Double Learning Note continued: 6.8.Games, Afterstates, and Other Special Cases -- 6.9.Summary -- 7.n-step Bootstrapping -- 7.1.n-step TD Prediction -- 7.2.n-step Sarsa -- 7.3.n-step Off-policy Learning -- 7.4.*Per-decision Methods with Control Variates -- 7.5.Off-policy Learning Without Importance Sampling: The n-step Tree Backup Algorithm -- 7.6.*A Unifying Algorithm: n-step Q(u) -- 7.7.Summary -- 8.Planning and Learning with Tabular Methods -- 8.1.Models and Planning -- 8.2.Dyna: Integrated Planning, Acting, and Learning -- 8.3.When the Model Is Wrong -- 8.4.Prioritized Sweeping -- 8.5.Expected vs. Sample Updates -- 8.6.Trajectory Sampling -- 8.7.Real-time Dynamic Programming -- 8.8.Planning at Decision Time -- 8.9.Heuristic Search -- 8.10.Rollout Algorithms -- 8.11.Monte Carlo Tree Search -- 8.12.Summary of the Chapter -- 8.13.Summary of Part I: Dimensions -- 9.On-policy Prediction with Approximation -- 9.1.Value-function Approximation -- 9.2.The Prediction Objective (VE) Note continued: 9.3.Stochastic-gradient and Semi-gradient Methods -- 9.4.Linear Methods -- 9.5.Feature Construction for Linear Methods -- 9.5.1.Polynomials -- 9.5.2.Fourier Basis -- 9.5.3.Coarse Coding -- 9.5.4.Tile Coding -- 9.5.5.Radial Basis Functions -- 9.6.Selecting Step-Size Parameters Manually -- 9.7.Nonlinear Function Approximation: Artificial Neural Networks -- 9.8.Least-Squares TD -- 9.9.Memory-based Function Approximation -- 9.10.Kernel-based Function Approximation -- 9.11.Looking Deeper at On-policy Learning: Interest and Emphasis -- 9.12.Summary -- 10.On-policy Control with Approximation -- 10.1.Episodic Semi-gradient Control -- 10.2.Semi-gradient n-step Sarsa -- 10.3.Average Reward: A New Problem Setting for Continuing Tasks -- 10.4.Deprecating the Discounted Setting -- 10.5.Differential Semi-gradient n-step Sarsa -- 10.6.Summary -- 11.*Off-policy Methods with Approximation -- 11.1.Semi-gradient Methods -- 11.2.Examples of Off-policy Divergence Note continued: 11.3.The Deadly Triad -- 11.4.Linear Value-function Geometry -- 11.5.Gradient Descent in the Bellman Error -- 11.6.The Bellman Error is Not Learnable -- 11.7.Gradient-TD Methods -- 11.8.Emphatic-TD Methods -- 11.9.Reducing Variance -- 11.10.Summary -- 12.Eligibility Traces -- 12.1.The A-return -- 12.2.TD(A) -- 12.3.n-step Truncated A-return Methods -- 12.4.Redoing Updates: Online A-return Algorithm -- 12.5.True Online TD(A) -- 12.6.*Dutch Traces in Monte Carlo Learning -- 12.7.Sarsa(A) -- 12.8.Variable A and ry -- 12.9.Off-policy Traces with Control Variates -- 12.10.Watkins's Q(A) to Tree-Backup(A) -- 12.11.Stable Off-policy Methods with Traces -- 12.12.Implementation Issues -- 12.13.Conclusions -- 13.Policy Gradient Methods -- 13.1.Policy Approximation and its Advantages -- 13.2.The Policy Gradient Theorem -- 13.3.REINFORCE: Monte Carlo Policy Gradient -- 13.4.REINFORCE with Baseline -- 13.5.Actor-Critic Methods Note continued: 13.6.Policy Gradient for Continuing Problems -- 13.7.Policy Parameterization for Continuous Actions -- 13.8.Summary -- 14.Psychology -- 14.1.Prediction and Control -- 14.2.Classical Conditioning -- 14.2.1.Blocking and Higher-order Conditioning -- 14.2.2.The Rescorla-Wagner Model -- 14.2.3.The TD Model -- 14.2.4.TD Model Simulations -- 14.3.Instrumental Conditioning -- 14.4.Delayed Reinforcement -- 14.5.Cognitive Maps -- 14.6.Habitual and Goal-directed Behavior -- 14.7.Summary -- 15.Neuroscience -- 15.1.Neuroscience Basics -- 15.2.Reward Signals, Reinforcement Signals, Values, and Prediction Errors -- 15.3.The Reward Prediction Error Hypothesis -- 15.4.Dopamine -- 15.5.Experimental Support for the Reward Prediction Error Hypothesis -- 15.6.TD Error/Dopamine Correspondence -- 15.7.Neural Actor-Critic -- 15.8.Actor and Critic Learning Rules -- 15.9.Hedonistic Neurons -- 15.10.Collective Reinforcement Learning -- 15.11.Model-based Methods in the Brain Note continued: 15.12.Addiction -- 15.13.Summary -- 16.Applications and Case Studies -- 16.1.TD-Gammon -- 16.2.Samuel's Checkers Player -- 16.3.Watson's Daily-Double Wagering -- 16.4.Optimizing Memory Control -- 16.5.Human-level Video Game Play -- 16.6.Mastering the Game of Go -- 16.6.1.AlphaGo -- 16.6.2.AlphaGo Zero -- 16.7.Personalized Web Services -- 16.8.Thermal Soaring -- 17.Frontiers -- 17.1.General Value Functions and Auxiliary Tasks -- 17.2.Temporal Abstraction via Options -- 17.3.Observations and State -- 17.4.Designing Reward Signals -- 17.5.Remaining Issues -- 17.6.Experimental Support for the Reward Prediction Error Hypothesis.},
author = {Sutton, Richard S. and Barto, Andrew G.},
edition = {2nd},
isbn = {9780262039246},
pages = {526},
publisher = {MIT press},
title = {{Reinforcement learning : an introduction}},
url = {https://mitpress.mit.edu/books/reinforcement-learning-second-edition},
year = {2018}
}
@article{Pohlen2018,
abstract = {Despite significant advances in the field of deep Reinforcement Learning (RL), today's algorithms still fail to learn human-level policies consistently over a set of diverse tasks such as Atari 2600 games. We identify three key challenges that any algorithm needs to master in order to perform well on all games: processing diverse reward distributions, reasoning over long time horizons, and exploring efficiently. In this paper, we propose an algorithm that addresses each of these challenges and is able to learn human-level policies on nearly all Atari games. A new transformed Bellman operator allows our algorithm to process rewards of varying densities and scales; an auxiliary temporal consistency loss allows us to train stably using a discount factor of {\$}\backslashgamma = 0.999{\$} (instead of {\$}\backslashgamma = 0.99{\$}) extending the effective planning horizon by an order of magnitude; and we ease the exploration problem by using human demonstrations that guide the agent towards rewarding states. When tested on a set of 42 Atari games, our algorithm exceeds the performance of an average human on 40 games using a common set of hyper parameters. Furthermore, it is the first deep RL algorithm to solve the first level of Montezuma's Revenge.},
archivePrefix = {arXiv},
arxivId = {1805.11593},
author = {Pohlen, Tobias and Piot, Bilal and Hester, Todd and Azar, Mohammad Gheshlaghi and Horgan, Dan and Budden, David and Barth-Maron, Gabriel and van Hasselt, Hado and Quan, John and Ve{\v{c}}er{\'{i}}k, Mel and Hessel, Matteo and Munos, R{\'{e}}mi and Pietquin, Olivier},
eprint = {1805.11593},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pohlen et al. - 2018 - Observe and Look Further Achieving Consistent Performance on Atari.pdf:pdf},
month = {may},
title = {{Observe and Look Further: Achieving Consistent Performance on Atari}},
url = {http://arxiv.org/abs/1805.11593},
year = {2018}
}
@article{Ilyas2018,
abstract = {We study how the behavior of deep policy gradient algorithms reflects the conceptual framework motivating their development. We propose a fine-grained analysis of state-of-the-art methods based on key aspects of this framework: gradient estimation, value prediction, optimization landscapes, and trust region enforcement. We find that from this perspective, the behavior of deep policy gradient algorithms often deviates from what their motivating framework would predict. Our analysis suggests first steps towards solidifying the foundations of these algorithms, and in particular indicates that we may need to move beyond the current benchmark-centric evaluation methodology.},
archivePrefix = {arXiv},
arxivId = {1811.02553},
author = {Ilyas, Andrew and Engstrom, Logan and Santurkar, Shibani and Tsipras, Dimitris and Janoos, Firdaus and Rudolph, Larry and Madry, Aleksander},
eprint = {1811.02553},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ilyas et al. - 2018 - Are Deep Policy Gradient Algorithms Truly Policy Gradient Algorithms.pdf:pdf},
month = {nov},
title = {{Are Deep Policy Gradient Algorithms Truly Policy Gradient Algorithms?}},
url = {http://arxiv.org/abs/1811.02553},
year = {2018}
}
@article{Gottesman2019,
abstract = {In this Comment, we provide guidelines for reinforcement learning for decisions about patient treatment that we hope will accelerate the rate at which observational cohorts can inform healthcare practice in a safe, risk-conscious manner.},
author = {Gottesman, Omer and Johansson, Fredrik and Komorowski, Matthieu and Faisal, Aldo and Sontag, David and Doshi-Velez, Finale and Celi, Leo Anthony},
doi = {10.1038/s41591-018-0310-5},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gottesman et al. - 2019 - Guidelines for reinforcement learning in healthcare.pdf:pdf},
issn = {1078-8956},
journal = {Nature Medicine},
keywords = {Health care,Machine learning},
month = {jan},
number = {1},
pages = {16--18},
publisher = {Nature Publishing Group},
title = {{Guidelines for reinforcement learning in healthcare}},
url = {http://www.nature.com/articles/s41591-018-0310-5},
volume = {25},
year = {2019}
}
@misc{Schlobach2018,
author = {Schlobach, M. and Retzer, S.},
booktitle = {Sustainable Transport in China},
title = {{Didi Chuxing - How China's ride-hailing leader aims to transform the future of mobility}},
url = {https://www.sustainabletransport.org/archives/6317},
urldate = {2020-07-29},
year = {2018}
}
@inproceedings{Schulman2016,
author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael I. and Abbeel, Pieter},
booktitle = {ICLR},
file = {:C$\backslash$:/Users/mg2289/Downloads/Schulman et al. - 2016 - High-Dimensional Continuous Control Using Generalized Advantage Estimation.pdf:pdf},
title = {{High-Dimensional Continuous Control Using Generalized Advantage Estimation}},
url = {https://www.semanticscholar.org/paper/High-Dimensional-Continuous-Control-Using-Advantage-Schulman-Moritz/1aa02f027663a626f3aa17ed9bab52377a23f634},
year = {2016}
}
@article{Ching-AnCheng2020,
author = {{Ching-An Cheng} and {Xinyan Yan} and {Byron Boots}},
journal = {Proceedings of Machine Learning Research},
pages = {1379--1394},
title = {{Trajectory-wise Control Variates for Variance Reduction in Policy Gradient Methods}},
url = {http://proceedings.mlr.press/v100/cheng20a.html},
volume = {100},
year = {2020}
}
@inproceedings{Chen2014a,
abstract = {In this paper, we investigate the control of non-conventional queueing networks, where multiple concurrent state transitions following non-exponential and general sojourn distributions are allowed. Two approximation schemes are discussed that produce an approximated Markovian model. We further propose to model the problem as an uncertain Markov decision processes (MDP) by considering the induced approximation error. A new simulation-based approach is investigated here, to give an overall optimal policy beyond the classic approach to such problems, e.g., a robust formulation. In particular, we approach the problem as one of finding a best overall control policy, via exploration and exploitation within a heuristic policy set. An approximate dynamic programming algorithm is then used, in connection with a parametric cost function, for efficiently learning and finding policies in this set. We show that the problem of finding the best control policy within this new policy set can be understood, equivalently, as one of finding the best set of parameters for one-stage cost function of the problem. Later, an integrated framework, denoted as extended actor-critic, is proposed to give a comprehensive treatment for those types of problems. Results of a case study are also presented and discussed.},
author = {Chen, Xiaoting and Fernandez, Emmanuel},
booktitle = {Proceedings of the 2014 Industrial and Systems Engineering Research Conference},
title = {{Control of Non-Conventional Queueing Networks: A Parametric and Approximate Dynamic Programming Approach}},
url = {https://pdfs.semanticscholar.org/7960/9c36c3a8db00f8f72832cd269a160785461e.pdf},
year = {2014}
}
@article{Lippman1975,
abstract = {We consider the problem of controlling M/M/c queuing systems. By providing a new definition of the time of transition, we enlarge the standard set of decision epochs and obtain a preferred version ...},
author = {Lippman, Steven A.},
doi = {10.1287/opre.23.4.687},
issn = {0030-364X},
journal = {Operations Research},
month = {aug},
number = {4},
pages = {687--710},
publisher = { INFORMS },
title = {{Applying a New Device in the Optimization of Exponential Queuing Systems}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/opre.23.4.687},
volume = {23},
year = {1975}
}
@inproceedings{Ramirez-Hernandez,
author = {Ramirez-Hernandez, J.A. and Fernandez, E.},
booktitle = {Proceedings of the 44th IEEE Conference on Decision and Control},
doi = {10.1109/CDC.2005.1582481},
isbn = {0-7803-9567-0},
pages = {2158--2163},
publisher = {IEEE},
title = {{A Case Study in Scheduling Reentrant Manufacturing Lines: Optimal and Simulation-Based Approaches}},
url = {http://ieeexplore.ieee.org/document/1582481/},
year = {2005}
}
@techreport{Abbasi-Yadkori,
abstract = {We study average and total cost Markov decision problems with large state spaces. Since the computational and statistical cost of finding the optimal policy scales with the size of the state space, we focus on searching for near-optimality in a low-dimensional family of policies. In particular , we show that for problems with a Kullback-Leibler divergence cost function, we can recast policy optimization as a convex optimization and solve it approximately using a stochastic subgra-dient algorithm. This method scales in complexity with the family of policies but not the state space. We show that the performance of the resulting policy is close to the best in the low-dimensional family. We demonstrate the efficacy of our approach by optimizing a policy for budget allocation in crowd labeling, an important crowd-sourcing application.},
author = {Abbasi-Yadkori, Yasin and Bartlett, Peter L and Chen, Xi and Malek, Alan},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abbasi-Yadkori et al. - Unknown - Large-Scale Markov Decision Problems with KL Control Cost and its Application to Crowdsourcing.pdf:pdf},
title = {{Large-Scale Markov Decision Problems with KL Control Cost and its Application to Crowdsourcing}}
}
@phdthesis{Liu2019,
author = {Liu, Bai S.M.},
pages = {61},
school = {Massachusetts Institute of Technology},
title = {{Reinforcement learning in network control}},
year = {2019}
}
@article{Nichol2018,
abstract = {In this report, we present a new reinforcement learning (RL) benchmark based on the Sonic the Hedgehog (TM) video game franchise. This benchmark is intended to measure the performance of transfer learning and few-shot learning algorithms in the RL domain. We also present and evaluate some baseline algorithms on the new benchmark.},
archivePrefix = {arXiv},
arxivId = {1804.03720},
author = {Nichol, Alex and Pfau, Vicki and Hesse, Christopher and Klimov, Oleg and Schulman, John},
eprint = {1804.03720},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nichol et al. - 2018 - Gotta Learn Fast A New Benchmark for Generalization in RL.pdf:pdf},
month = {apr},
title = {{Gotta Learn Fast: A New Benchmark for Generalization in RL}},
url = {http://arxiv.org/abs/1804.03720},
year = {2018}
}
@article{Haarnoja2018a,
author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
journal = {Proceedings of Machine Learning Research},
pages = {1861--1870},
title = {{Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}},
url = {http://proceedings.mlr.press/v80/haarnoja18b.html},
volume = {80},
year = {2018}
}
@inproceedings{Ramirez-Hernandez2009,
author = {Ramirez-Hernandez, Jose A. and Fernandez, Emmanuel},
booktitle = {Proceedings of the 2009 Winter Simulation Conference (WSC)},
doi = {10.1109/WSC.2009.5429179},
isbn = {978-1-4244-5770-0},
month = {dec},
pages = {1634--1645},
publisher = {IEEE},
title = {{A simulation-based Approximate Dynamic Programming approach for the control of the Intel Mini-Fab benchmark model}},
url = {http://ieeexplore.ieee.org/document/5429179/},
year = {2009}
}
@inproceedings{Glorot2010,
author = {Glorot, Xavier and Bengio, Yoshua},
booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
pages = {249--256},
title = {{Understanding the difficulty of training deep feedforward neural networks}},
url = {http://proceedings.mlr.press/v9/glorot10a.html},
year = {2010}
}
@article{Ho2020,
author = {Ho, Dean},
doi = {10.1126/science.aaz3023},
file = {:C$\backslash$:/Users/mg2289/Downloads/982.full.pdf:pdf},
journal = {Science},
number = {6481},
pages = {982--983},
title = {{Artificial intelligence in cancer therapy}},
volume = {367},
year = {2020}
}
@article{Maglaras2000,
author = {Maglaras, Constantinos},
doi = {10.1214/aoap/1019487513},
issn = {1050-5164},
journal = {The Annals of Applied Probability},
keywords = {Queueing networks,asymptotic optimality,discrete-review policies,fluid models,trajectory tracking},
month = {aug},
number = {3},
pages = {897--929},
publisher = {Institute of Mathematical Statistics},
title = {{Discrete-review policies for scheduling stochastic networks: trajectory tracking and fluid-scale asymptotic optimality}},
url = {http://projecteuclid.org/euclid.aoap/1019487513},
volume = {10},
year = {2000}
}
@article{Wang2016,
abstract = {This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.},
archivePrefix = {arXiv},
arxivId = {1611.01224},
author = {Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and de Freitas, Nando},
eprint = {1611.01224},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2016 - Sample Efficient Actor-Critic with Experience Replay.pdf:pdf},
title = {{Sample Efficient Actor-Critic with Experience Replay}},
url = {http://arxiv.org/abs/1611.01224},
year = {2016}
}
@article{Bauerle2001,
author = {B{\"{a}}uerle, Nicole},
doi = {10.1214/aoap/1019487606},
issn = {1050-5164},
journal = {The Annals of Applied Probability},
keywords = {Markov decision process,Stochastic network,fluid model,stochastic orderings,weak convergence},
number = {4},
pages = {1065--1083},
publisher = {Institute of Mathematical Statistics},
title = {{Asymptotic optimality of tracking policies in stochastic networks}},
url = {http://projecteuclid.org/euclid.aoap/1019487606},
volume = {10},
year = {2001}
}
@article{Roberts1998b,
author = {Roberts, Gareth O. and Rosenthal, Jeffrey S. and Schwartz, Peter O.},
journal = {Journal of Applied Probability},
number = {1},
pages = {1--11},
title = {{Convergence Properties of Perturbed Markov Chains on JSTOR}},
url = {https://www.jstor.org/stable/3215541?seq=1{\#}metadata{\_}info{\_}tab{\_}contents},
volume = {35},
year = {1998}
}
@article{Bertsimas2015,
author = {Bertsimas, Dimitris and Nasrabadi, Ebrahim and Paschalidis, Ioannis Ch.},
doi = {10.1109/TAC.2014.2352711},
issn = {0018-9286},
journal = {IEEE Transactions on Automatic Control},
number = {3},
pages = {715--728},
title = {{Robust Fluid Processing Networks}},
url = {http://ieeexplore.ieee.org/document/6887315/},
volume = {60},
year = {2015}
}
@book{Puterman2005,
abstract = {Originally published: New York, N.Y. : John Wiley {\&} Sons, 1994. 1. Introduction -- 2. Model formulation -- 3. Examples -- 4. Finite-horizon Markov decision processes -- 5. Infinite-horizon models : foundations -- 6. Discounted Markov decision problems -- 7. The expected total-reward criterion -- 8. Average reward and related criteria -- 9. The average reward criterion-multichain and communicating models -- 10. Sensitive discount optimality -- 11. Continuous-time models -- App. A. Markov chains -- App. B. Semicontinuous functions -- App. C. Normed linear spaces -- App. D. Linear programming.},
author = {Puterman, Martin L.},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Puterman - 2005 - Markov decision processes discrete stochastic dynamic programming.pdf:pdf},
isbn = {0471727822},
pages = {649},
publisher = {Wiley-Interscience},
title = {{Markov decision processes : discrete stochastic dynamic programming}},
year = {2005}
}
@article{Henderson2003,
abstract = {This paper concerns modeling and policy synthesis for regulation of multiclass queueing networks. A 2-parameter network model is introduced to allow independent modeling of variability and mean processing-rates, while maintaining simplicity of the model. Policy synthesis is based on consideration of more tractable workload models, and then translating a policy from this abstraction to the discrete network of interest. Translation is made possible through the use of safety-stocks that maintain feasibility of workload trajectories. This is a well-known approach in the queueing theory literature, and may be viewed as a generic approach to avoid deadlock in a discrete-event dynamical system. Simulation is used to evaluate a given policy, and to tune safety-stock levels. These simulations are accelerated through a variance reduction technique that incorporates stochastic approximation to tune the variance reduction. The search for appropriate safety-stock levels is coordinated through a cutting plane algorithm. Both the policy synthesis and the simulation acceleration rely heavily on the development of approximations to the value function through fluid model considerations.},
author = {Henderson, Shane G. and Meyn, Sean P. and Tadi{\'{c}}, Vladislav B.},
doi = {10.1023/A:1022197004856},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Henderson, Meyn, Tadi{\'{c}} - 2003 - Performance evaluation and policy selection in multiclass networks.pdf:pdf},
issn = {09246703},
journal = {Discrete Event Dynamic Systems: Theory and Applications},
month = {jan},
number = {1-2},
pages = {149--189},
title = {{Performance evaluation and policy selection in multiclass networks}},
volume = {13},
year = {2003}
}
@inproceedings{Thomas2014,
abstract = {We show that several popular discounted reward natural actor-critics, including the popular NAC-LSTD and eNAC algorithms, do not generate un-biased estimates of the natural policy gradient as claimed. We derive the first unbiased discounted reward natural actor-critics using batch and iterative approaches to gradient estimation. We argue that the bias makes the existing algorithms more appropriate for the average reward setting. We also show that, when Sarsa($\lambda$) is guaranteed to converge to an optimal policy, the objective function used by natural actor-critics has only global optima, so policy gradient methods are guaranteed to converge to globally optimal policies as well.},
author = {Thomas, Philip S},
booktitle = {Proceedings of the 31 st International Conference on Machine Learning},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thomas - 2014 - Bias in Natural Actor-Critic Algorithms.pdf:pdf},
title = {{Bias in Natural Actor-Critic Algorithms}},
url = {http://proceedings.mlr.press/v32/thomas14.pdf},
year = {2014}
}
@article{Zahavy2016,
abstract = {In recent years there is a growing interest in using deep representations for reinforcement learning. In this paper, we present a methodology and tools to analyze Deep Q-networks (DQNs) in a non-blind matter. Moreover, we propose a new model, the Semi Aggregated Markov Decision Process (SAMDP), and an algorithm that learns it automatically. The SAMDP model allows us to identify spatio-temporal abstractions directly from features and may be used as a sub-goal detector in future work. Using our tools we reveal that the features learned by DQNs aggregate the state space in a hierarchical fashion, explaining its success. Moreover, we are able to understand and describe the policies learned by DQNs for three different Atari2600 games and suggest ways to interpret, debug and optimize deep neural networks in reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1602.02658},
author = {Zahavy, Tom and Zrihem, Nir Ben and Mannor, Shie},
eprint = {1602.02658},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zahavy, Zrihem, Mannor - 2016 - Graying the black box Understanding DQNs.pdf:pdf},
month = {feb},
title = {{Graying the black box: Understanding DQNs}},
url = {http://arxiv.org/abs/1602.02658},
year = {2016}
}
@inproceedings{Liu2018,
abstract = {Policy gradient methods have achieved remarkable successes in solving challenging reinforcement learning problems. However, it still often suffers from the large variance issue on policy gradient estimation, which leads to poor sample efficiency during training. In this work, we propose a control variate method to effectively reduce variance for policy gradient methods. Motivated by the Stein's identity, our method extends the previous control variate methods used in REINFORCE and advantage actor-critic by introducing more general action-dependent baseline functions. Empirical studies show that our method significantly improves the sample efficiency of the state-of-the-art policy gradient approaches.},
archivePrefix = {arXiv},
arxivId = {1710.11198},
author = {Liu, Hao and Feng, Yihao and Mao, Yi and Zhou, Dengyong and Peng, Jian and Liu, Qiang},
booktitle = {International Conference on Learning Representations},
eprint = {1710.11198},
month = {oct},
title = {{Action-depedent Control Variates for Policy Optimization via Stein's Identity}},
url = {http://arxiv.org/abs/1710.11198},
year = {2018}
}
@article{Bertsimas2011,
abstract = {Performance analysis of queueing networks is one of the most challenging areas of queueing theory. Barring very specialized models such as product-form type queueing networks, there exist very few results that provide provable nonasymptotic upper and lower bounds on key performance measures. In this paper we propose a new performance analysis method, which is based on the robust optimization. The basic premise of our approach is as follows: rather than assuming that the stochastic primitives of a queueing model satisfy certain probability laws-such as i.i.d. interarrival and service times distributions-we assume that the underlying primitives are deterministic and satisfy the implications of such probability laws. These implications take the form of simple linear constraints, namely, those motivated by the law of the iterated logarithm (LIL). Using this approach we are able to obtain performance bounds on some key performance measures. Furthermore, these performance bounds imply similar bounds in the underlying stochastic queueing models. We demonstrate our approach on two types of queueing networks: (a) tandem single-class queueing network and (b) multiclass single-server queueing network. In both cases, using the proposed robust optimization approach, we are able to obtain explicit upper bounds on some steady-state performance measures. For example, for the case of TSC system we obtain a bound of the form C(1-p)-1 ln ln C(1-p) -1) on the expected steady-state sojourn time, where C is an explicit constant and P is the bottleneck traffic intensity. This qualitatively agrees with the correct heavy traffic scaling of this performance measure up to the ln ln C(1-p)-1)correction factor. {\textcopyright} 2011 INFORMS.},
author = {Bertsimas, Dimitris and Gamarnik, David and {Anatoliy Rikun}, Alexander},
doi = {10.1287/opre.1100.0879},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bertsimas, Gamarnik, Anatoliy Rikun - 2011 - Performance analysis of queueing networks via robust optimization.pdf:pdf},
issn = {0030364X},
journal = {Operations Research},
keywords = {Applications,Linear applications,Optimization,Probability,Programming,Queues,Tandem},
number = {2},
pages = {455--466},
publisher = {Alexander Anatoliy Rikun},
title = {{Performance analysis of queueing networks via robust optimization}},
url = {http://pubsonline.informs.org466.https//doi.org/10.1287/opre.1100.0879http://www.informs.org},
volume = {59},
year = {2011}
}
@article{Kartashov1986,
author = {Kartashov, N. V.},
doi = {10.1007/BF01089787},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kartashov - 1986 - Strongly stable Markov chains.pdf:pdf},
issn = {00904104},
journal = {Journal of Soviet Mathematics},
month = {jul},
number = {2},
pages = {1493--1498},
publisher = {Kluwer Academic Publishers-Plenum Publishers},
title = {{Strongly stable Markov chains}},
volume = {34},
year = {1986}
}
@book{Asmussen2003,
author = {Asmussen, S{\o}ren},
booktitle = {Applied Probability and Queues},
doi = {10.1007/b97236},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Asmussen - 2003 - Applied Probability and Queues.pdf:pdf},
pages = {438},
publisher = {Springer New York},
title = {{Applied Probability and Queues}},
year = {2003}
}
@article{Zhang2020,
abstract = {Model reduction of Markov processes is a basic problem in modeling state-transition systems. Motivated by the state aggregation approach rooted in control theory, we study the statistical state compression of a discrete-state Markov chain from empirical trajectories. Through the lens of spectral decomposition, we study the rank and features of Markov processes, as well as properties like representability, aggregability, and lumpability. We develop spectral methods for estimating the transition matrix of a low-rank Markov model, estimating the leading subspace spanned by Markov features, and recovering latent structures like state aggregation and lumpable partition of the state space. We prove statistical upper bounds for the estimation errors and nearly matching minimax lower bounds. Numerical studies are performed on synthetic data and a dataset of New York City taxi trips.},
archivePrefix = {arXiv},
arxivId = {1802.02920},
author = {Zhang, Anru and Wang, Mengdi},
doi = {10.1109/TIT.2019.2956737},
eprint = {1802.02920},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Wang - 2020 - Spectral State Compression of Markov Processes.pdf:pdf},
issn = {15579654},
journal = {IEEE Transactions on Information Theory},
keywords = {Computational complexity,maximum likelihood estimation,minimax techniques,signal denoising,tensor SVD},
month = {may},
number = {5},
pages = {3202--3231},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Spectral State Compression of Markov Processes}},
volume = {66},
year = {2020}
}
@article{Schulman2017,
abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
archivePrefix = {arXiv},
arxivId = {1707.06347},
author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
eprint = {1707.06347},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:pdf},
title = {{Proximal Policy Optimization Algorithms}},
url = {http://arxiv.org/abs/1707.06347},
year = {2017}
}
@article{Luong2019,
abstract = {This paper presents a comprehensive literature review on applications of deep reinforcement learning (DRL) in communications and networking. Modern networks, e.g., Internet of Things (IoT) and unmanned aerial vehicle (UAV) networks, become more decentralized and autonomous. In such networks, network entities need to make decisions locally to maximize the network performance under uncertainty of network environment. Reinforcement learning has been efficiently used to enable the network entities to obtain the optimal policy including, e.g., decisions or actions, given their states when the state and action spaces are small. However, in complex and large-scale networks, the state and action spaces are usually large, and the reinforcement learning may not be able to find the optimal policy in reasonable time. Therefore, DRL, a combination of reinforcement learning with deep learning, has been developed to overcome the shortcomings. In this survey, we first give a tutorial of DRL from fundamental concepts to advanced models. Then, we review DRL approaches proposed to address emerging issues in communications and networking. The issues include dynamic network access, data rate control, wireless caching, data offloading, network security, and connectivity preservation which are all important to next generation networks, such as 5G and beyond. Furthermore, we present applications of DRL for traffic routing, resource sharing, and data collection. Finally, we highlight important challenges, open issues, and future research directions of applying DRL.},
archivePrefix = {arXiv},
arxivId = {1810.07862},
author = {Luong, Nguyen Cong and Hoang, Dinh Thai and Gong, Shimin and Niyato, Dusit and Wang, Ping and Liang, Ying Chang and Kim, Dong In},
doi = {10.1109/COMST.2019.2916583},
eprint = {1810.07862},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Luong et al. - 2019 - Applications of Deep Reinforcement Learning in Communications and Networking A Survey.pdf:pdf},
issn = {1553877X},
journal = {IEEE Communications Surveys and Tutorials},
keywords = {Deep reinforcement learning,caching,communications,data collection,data offloading,deep Q-learning,networking,rate control,security,spectrum access},
number = {4},
pages = {3133--3174},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Applications of Deep Reinforcement Learning in Communications and Networking: A Survey}},
volume = {21},
year = {2019}
}
@article{Fox1989,
abstract = {We numerically estimate, via simulation, the expected infinite-horizon discounted cost d of running a stochastic system. A naive strategy estimates a finite-horizon approximation to d. We propose alternatives. All are ranked with respect to asymptotic variance as a function of computer-time budget and discount rate, when semi-Markov and/or regenerative structure or neither is assumed. In this setting, the naive truncation estimator loses; it may triumph, however, when the computer-time budget is modest, the discount rate is large, and the process simulated is not regenerative or has long cycle lengths.},
author = {Fox, Bennett L. and Glynn, Peter W.},
doi = {10.2307/2632279},
journal = {Management Science},
pages = {1297--1315},
publisher = {INFORMS},
title = {{Simulating Discounted Costs}},
url = {https://www.jstor.org/stable/2632279},
volume = {35},
year = {1989}
}
@techreport{Braylan2015,
abstract = {We show that setting a reasonable frame skip can be critical to the performance of agents learning to play Atari 2600 games. In all of the six games in our experiments, frame skip is a strong determinant of success. For two of these games, setting a large frame skip leads to state-of-the-art performance. The rate at which an agent interacts with its environment may be critical to its success. In the Arcade Learning Environment (ALE) (Bellemare et al. 2013) games run at sixty frames per second, and agents can submit an action at every frame. Frame skip is the number of frames an action is repeated before a new action is selected. Existing reinforcement learning (RL) approaches use static frame skip: HNEAT (Hausknecht et al. 2013) uses a frame skip of 0; DQN (Mnih et al. 2013) uses a frame skip of 2-3; SARSA and planning approaches (Bellemare et al. 2013) use a frame skip of 4. When action selection is computationally intensive , setting a higher frame skip can significantly decrease the time it takes to simulate an episode, at the cost of missing opportunities that only exist at a finer resolution. A large frame skip can also prevent degenerate superhuman reflex strategies, such as those described by Hausknecht et al. for Bowling, Kung Fu Master, Video Pinball and Beam Rider. We show that in addition to these advantages agents that act with high frame skip can actually learn faster with respect to the number of training episodes than those that skip no frames. We present results for six of the seven games covered by Mnih et al.: three (Beam Rider, Breakout and Pong) for which DQN was able to achieve near-or superhuman performance, and three (Q*Bert, Space Invaders and Seaquest) for which all RL approaches are far from human performance. These latter games were understood to be difficult because they require 'strategy that extends over long time scales.},
author = {Braylan, Alex and Hollenbeck, Mark and Meyerson, Elliot and Miikkulainen, Risto},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Braylan et al. - 2015 - Frame Skip Is a Powerful Parameter for Learning to Play Atari.pdf:pdf},
keywords = {AAAI Technical Report WS-15-10},
title = {{Frame Skip Is a Powerful Parameter for Learning to Play Atari}},
url = {www.aaai.org},
year = {2015}
}
@inproceedings{Chang-chunLiu,
author = {{Chang-chun Liu} and {Hui-yu Jin} and {Yu Tian} and {Hai-bin Yu}},
booktitle = {2001 International Conferences on Info-Tech and Info-Net. Proceedings (Cat. No.01EX479)},
doi = {10.1109/ICII.2001.983070},
isbn = {0-7803-7010-4},
pages = {280--285},
publisher = {IEEE},
title = {{Reinforcement learning approach to re-entrant manufacturing system scheduling}},
url = {http://ieeexplore.ieee.org/document/983070/},
volume = {3},
year = {2001}
}
@phdthesis{Marbach1998,
abstract = {Thesis (Ph. D.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1998.},
author = {Marbach, Peter},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Marbach - 1998 - Simulation-based optimization of Markov decision processes(2).pdf:pdf},
keywords = {Electrical Engineering and Computer Science,Thesis},
pages = {169},
publisher = {Massachusetts Institute of Technology},
school = {Massachusetts Institute of Technology},
title = {{Simulation-based optimization of Markov decision processes}},
year = {1998}
}
@article{Laws1990,
abstract = {{\textless}p{\textgreater}This paper is concerned with the problem of optimally scheduling a multiclass open queueing network with four single-server stations in which dynamic control policies are permitted. Under the assumption that the system is heavily loaded, the original scheduling problem can be approximated by a dynamic control problem involving Brownian motion. We reformulate and solve this problem and, from the interpretation of the solution, we obtain two dynamic scheduling policies for our queueing network. We compare the performance of these policies with two static scheduling policies and a lower bound via simulation. Our results suggest that under either dynamic policy the system, at least when heavily loaded, exhibits the form of resource pooling given by the solution to the approximating control problem. Furthermore, even when lightly loaded the system performs better under the dynamic policies than under either static policy.{\textless}/p{\textgreater}},
author = {Laws, C. N. and Louth, G. M.},
doi = {10.1017/S0269964800001492},
issn = {0269-9648},
journal = {Probability in the Engineering and Informational Sciences},
month = {jan},
number = {1},
pages = {131--156},
publisher = {Cambridge University Press},
title = {{Dynamic Scheduling of a Four-Station Queueing Network}},
url = {https://www.cambridge.org/core/product/identifier/S0269964800001492/type/journal{\_}article},
volume = {4},
year = {1990}
}
@book{Gallager1996a,
address = {Boston, MA},
author = {Gallager, Robert G.},
doi = {10.1007/978-1-4615-2329-1},
isbn = {978-1-4613-5986-9},
publisher = {Springer US},
title = {{Discrete Stochastic Processes}},
url = {http://link.springer.com/10.1007/978-1-4615-2329-1},
year = {1996}
}
@article{Roberts1998,
abstract = {In this paper, we consider the question of which convergence properties of Markov chains are preserved under small perturbations. Properties considered include geometric ergodicity and rates of convergence. Perturbations considered include roundoff error from computer simulation. We are motivated primarily by interest in Markov chain Monte Carlo algorithms. {\textcopyright} 1998 Applied Probability Trust.},
author = {Roberts, Gareth O. and Rosenthal, Jeffrey S. and Schwartz, Peter O.},
doi = {10.1239/jap/1032192546},
issn = {00219002},
journal = {Journal of Applied Probability},
keywords = {Convergence rate,Geometric convergence,Markov chain,Monte Carlo,Perturbation,Roundoff error},
month = {jan},
number = {1},
pages = {1--11},
title = {{Convergence properties of perturbed Markov chains}},
volume = {35},
year = {1998}
}
@article{Moritz2018,
abstract = {The next generation of AI applications will continuously interact with the environment and learn from these interactions. These applications impose new and demanding systems requirements, both in terms of performance and flexibility. In this paper, we consider these requirements and present Ray---a distributed system to address them. Ray implements a unified interface that can express both task-parallel and actor-based computations, supported by a single dynamic execution engine. To meet the performance requirements, Ray employs a distributed scheduler and a distributed and fault-tolerant store to manage the system's control state. In our experiments, we demonstrate scaling beyond 1.8 million tasks per second and better performance than existing specialized systems for several challenging reinforcement learning applications.},
archivePrefix = {arXiv},
arxivId = {1712.05889},
author = {Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I. and Stoica, Ion},
eprint = {1712.05889},
title = {{Ray: A Distributed Framework for Emerging AI Applications}},
url = {http://arxiv.org/abs/1712.05889},
year = {2018}
}
@article{Salimans2018,
abstract = {We propose a new method for learning from a single demonstration to solve hard exploration tasks like the Atari game Montezuma's Revenge. Instead of imitating human demonstrations, as proposed in other recent works, our approach is to maximize rewards directly. Our agent is trained using off-the-shelf reinforcement learning, but starts every episode by resetting to a state from a demonstration. By starting from such demonstration states, the agent requires much less exploration to learn a game compared to when it starts from the beginning of the game at every episode. We analyze reinforcement learning for tasks with sparse rewards in a simple toy environment, where we show that the run-time of standard RL methods scales exponentially in the number of states between rewards. Our method reduces this to quadratic scaling, opening up many tasks that were previously infeasible. We then apply our method to Montezuma's Revenge, for which we present a trained agent achieving a high-score of 74,500, better than any previously published result.},
archivePrefix = {arXiv},
arxivId = {1812.03381},
author = {Salimans, Tim and Chen, Richard},
eprint = {1812.03381},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Salimans, Chen - 2018 - Learning Montezuma's Revenge from a Single Demonstration.pdf:pdf},
month = {dec},
title = {{Learning Montezuma's Revenge from a Single Demonstration}},
url = {http://arxiv.org/abs/1812.03381},
year = {2018}
}
@article{Silver2017,
abstract = {Starting from zero knowledge and without human data, AlphaGo Zero was able to teach itself to play Go and to develop novel strategies that provide new insights into the oldest of games.},
author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature24270},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
keywords = {Computational science,Computer science,Reward},
number = {7676},
pages = {354--359},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go without human knowledge}},
url = {http://www.nature.com/doifinder/10.1038/nature24270},
volume = {550},
year = {2017}
}
@article{Harrison1989,
author = {Harrison, J. Michael and Wein, Lawrence M.},
doi = {10.1007/BF01225319},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Harrison, Wein - 1989 - Scheduling networks of queues Heavy traffic analysis of a simple open network.pdf:pdf},
issn = {0257-0130},
journal = {Queueing Systems},
month = {dec},
number = {4},
pages = {265--279},
publisher = {Kluwer Academic Publishers},
title = {{Scheduling networks of queues: Heavy traffic analysis of a simple open network}},
url = {http://link.springer.com/10.1007/BF01225319},
volume = {5},
year = {1989}
}
@article{Dellaportas2012,
author = {Dellaportas, Petros and Kontoyiannis, Ioannis},
doi = {10.1111/j.1467-9868.2011.01000.x},
issn = {13697412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
month = {jan},
number = {1},
pages = {133--161},
title = {{Control variates for estimation based on reversible Markov chain Monte Carlo samplers}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2011.01000.x},
volume = {74},
year = {2012}
}
@book{Wagner1975,
author = {Wagner, Harvey M.},
edition = {2nd},
pages = {1039 },
publisher = {Englewood Cliffs, N.J. : Prentice-Hall},
title = {{Principles of Operations Research: with applications to managerial decisions}},
url = {https://newcatalog.library.cornell.edu/catalog/468158},
year = {1975}
}
@article{Bauerle2002,
author = {B{\"{a}}uerle, Nicole},
journal = { Advances in Applied Probability},
number = {2},
pages = {313--328},
title = {{Optimal Control of Queueing Networks: An Approach via Fluid Models}},
url = {http://link.springer.com/10.1007/BF01149168},
volume = {34},
year = {2002}
}
@incollection{Harrison1988,
author = {Harrison, J. Michael},
doi = {10.1007/978-1-4613-8762-6_11},
pages = {147--186},
publisher = {Springer, New York, NY},
title = {{Brownian Models of Queueing Networks with Heterogeneous Customer Populations}},
url = {http://link.springer.com/10.1007/978-1-4613-8762-6{\_}11},
year = {1988}
}
@article{Taylor1993,
author = {Taylor, L. M. and Williams, R. J.},
doi = {10.1007/BF01292674},
issn = {0178-8051},
journal = {Probability Theory and Related Fields},
month = {sep},
number = {3},
pages = {283--317},
publisher = {Springer-Verlag},
title = {{Existence and uniqueness of semimartingale reflecting Brownian motions in an orthant}},
url = {http://link.springer.com/10.1007/BF01292674},
volume = {96},
year = {1993}
}
@article{OpenAI2019a,
abstract = {On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.},
archivePrefix = {arXiv},
arxivId = {1912.06680},
author = {OpenAI},
eprint = {1912.06680},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/OpenAI et al. - 2019 - Dota 2 with Large Scale Deep Reinforcement Learning(2).pdf:pdf},
month = {dec},
title = {{Dota 2 with Large Scale Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1912.06680},
year = {2019}
}
@inproceedings{Abadi2016,
author = {Abadi, Mart{\'{i}}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
booktitle = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
isbn = {9781931971331},
pages = {265--283},
publisher = {USENIX Association},
title = {{TensorFlow: A System for Large-Scale Machine Learning}},
url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi},
year = {2016}
}
@article{Henderson2002a,
abstract = {"Knowledge of either analytical or numerical approximations should enable more efficient simulation estimators to be constructed." This principle seems intuitively plausible and certainly attractive, yet no completely satisfactory general methodology has been developed to exploit it. The authors present a new approach for obtaining variance reduction in Markov process simulation that is applicable to a vast array of different performance measures. The approach relies on the construction of a martingale that is then used as an internal control variate.},
author = {Henderson, Shane G. and Glynn, Peter W.},
doi = {10.1287/moor.27.2.253.329},
issn = {0364765X},
journal = {Mathematics of Operations Research},
keywords = {Markov process,Martingale,Simulation,Variance reduction},
number = {2},
pages = {253--271},
publisher = {INFORMS Inst.for Operations Res.and the Management Sciences},
title = {{Approximating martingales for variance reduction in Markov process simulation}},
volume = {27},
year = {2002}
}
@misc{OpenAI2019,
author = {OpenAI},
doi = {https://blog.openai.com/openai-five/},
title = {{OpenAI Five}},
url = {https://openai.com/five/},
urldate = {2019-05-29},
year = {2019}
}
@article{Marbach2003,
abstract = {We consider a discrete time, finite state Markov reward process that depends on a set of parameters. We start with a brief review of (stochastic) gradient descent methods that tune the parameters in order to optimize the average reward, using a single (possibly simulated) sample path of the process of interest. The resulting algorithms can be implemented online, and have the property that the gradient of the average reward converges to zero with probability 1. On the other hand, the updates can have a high variance, resulting in slow convergence. We address this issue and propose two approaches to reduce the variance. These approaches rely on approximate gradient formulas, which introduce an additional bias into the update direction. We derive bounds for the resulting bias terms and characterize the asymptotic behavior of the resulting algorithms. For one of the approaches considered, the magnitude of the bias term exhibits an interesting dependence on the time it takes for the rewards to reach steady-state. We also apply the methodology to Markov reward processes with a reward-free termination state, and an expected total reward criterion. We use a call admission control problem to illustrate the performance of the proposed algorithms.},
author = {Marbach, Peter and Tsitsiklis, John N.},
doi = {10.1023/A:1022145020786},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Marbach, Tsitsiklis - 2003 - Approximate gradient methods in policy-space optimization of Markov reward processes.pdf:pdf},
issn = {09246703},
journal = {Discrete Event Dynamic Systems: Theory and Applications},
keywords = {Markov reward processes,Policy-space optimization,Simulation-based optimization},
month = {jan},
number = {1-2},
pages = {111--148},
title = {{Approximate gradient methods in policy-space optimization of Markov reward processes}},
volume = {13},
year = {2003}
}
@book{Gallager1996,
address = {Boston, MA},
author = {Gallager, Robert G.},
doi = {10.1007/978-1-4615-2329-1},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gallager - 1996 - Discrete Stochastic Processes.pdf:pdf},
isbn = {978-1-4613-5986-9},
publisher = {Springer US},
title = {{Discrete Stochastic Processes}},
url = {http://link.springer.com/10.1007/978-1-4615-2329-1},
year = {1996}
}
@article{DeFarias2003a,
abstract = {The curse of dimensionality gives rise to prohibitive computational requirements that render infeasible the exact solution of large-scale stochastic control problems. We study an efficient method b...},
author = {de Farias, D. P. and {Van Roy}, B.},
doi = {10.1287/opre.51.6.850.24925},
journal = {Operations Research},
number = {6},
pages = {850--865},
publisher = {INFORMS},
title = {{The Linear Programming Approach to Approximate Dynamic Programming}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/opre.51.6.850.24925},
volume = {51},
year = {2003}
}
@article{Bramson1996,
abstract = {Fluid models have recently become an important tool for the study of open multiclass queueing networks. We are interested in a family of such models, which we refer to as head-of-the-line proportional processor sharing (HLPPS) fluid models. Here, the fraction of time spent serving a class present at a station is proportional to the quantity of the class there, with all of the service going into the "first customer" of each class. To study such models, we employ an entropy function associated with the state of the system. The corresponding estimates show that if the traffic intensity function is at most 1, then such fluid models converge exponentially fast to equilibria. When the traffic intensity function is strictly less than 1, the limit is always the empty state and occurs after a finite time. A consequence is that generalized HLPPS networks with traffic intensity strictly less than 1 are positive Harris recurrent. Related results for FIFO fluid models of Kelly type were obtained in Bramson [4].},
author = {Bramson, Maury},
doi = {10.1007/bf01206549},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bramson - 1996 - Convergence to equilibria for fluid models of head-of-the-line proportional processor sharing queueing networks.pdf:pdf},
issn = {15729443},
journal = {Queueing Systems},
keywords = {Entropy,Equilibria,Fluid models,Queueing networks},
number = {1-4},
pages = {1--26},
publisher = {Kluwer Academic Publishers},
title = {{Convergence to equilibria for fluid models of head-of-the-line proportional processor sharing queueing networks}},
volume = {23},
year = {1996}
}
@inproceedings{Thicker2018,
abstract = {Policy gradient methods are a widely used class of model-free reinforcement learning algorithms where a state-dependent baseline is used to reduce gradient estimator variance. Several recent papers extend the baseline to depend on both the state and action and suggest that this significantly reduces variance and improves sample efficiency without introducing bias into the gradient estimates. To better understand this development, we decompose the variance of the policy gradient estimator and numerically show that learned state-action-dependent baselines do not in fact reduce variance over a state-dependent baseline in commonly tested benchmark domains. We confirm this unexpected result by reviewing the open-source code accompanying these prior papers, and show that subtle implementation decisions cause deviations from the methods presented in the papers and explain the source of the previously observed empirical gains. Furthermore, the variance decomposition highlights areas for improvement, which we demonstrate by illustrating a simple change to the typical value function parameterization that can significantly improve performance. Copyright 2018 by the author(s).},
archivePrefix = {arXiv},
arxivId = {1802.10031},
author = {Thicker, George and Bhupatiraju, Surya and Gu, Shixiang and Turner, Richard E. and Ghahramani, Zoubin and Levine, Sergey},
booktitle = {35th International Conference on Machine Learning, ICML 2018},
eprint = {1802.10031},
isbn = {9781510867963},
pages = {7985--7994},
publisher = {International Machine Learning Society (IMLS)},
title = {{The mirage of action-dependent baselines in reinforcement learning}},
volume = {11},
year = {2018}
}
@misc{DeepMind2016a,
author = {Gao, J. and Evans, R.},
title = {{DeepMind AI Reduces Google Data Centre Cooling Bill by 40{\%}}},
url = {https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/},
urldate = {2019-05-29},
year = {2016}
}
@inproceedings{Kakade2001,
author = {Kakade, Sham},
booktitle = {COLT '01/EuroCOLT '01},
doi = {10.1007/3-540-44581-1_40},
pages = {605--615},
title = {{Optimizing Average Reward Using Discounted Rewards}},
url = {http://link.springer.com/10.1007/3-540-44581-1{\_}40},
year = {2001}
}
@article{Adan2018,
abstract = {We study a parallel queueing system with multiple types of servers and customers. A bipartite graph describes which pairs of customer-server types are compatible. We consider the service policy that always assigns servers to the first, longest waiting compatible customer, and that always assigns customers to the longest idle compatible server if on arrival multiple compatible servers are available. For a general renewal stream of arriving customers, general service time distributions that depend both on customer and on server types, and general customer patience distributions, the behavior of such systems is very complicated. Key quantities for their performance are the matching rates, the fraction of services for each pair of compatible customer-server. Calculation of these matching rates in general is intractable, it depends on the entire shape of service time distributions. We suggest through a heuristic argument that if the number of servers becomes large, the matching rates are well approximated by matching rates calculated from the tractable bipartite infinite matching model. We present simulation evidence to support this heuristic argument, and show how this can be used to design systems with desired performance requirements.},
author = {Adan, Ivo J B F and Boon, Marko A A and Weiss, Gideon},
doi = {10.1016/j.ejor.2018.08.042},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Adan, Boon, Weiss - 2018 - Stochastics and Statistics Design heuristic for parallel many server systems R.pdf:pdf},
journal = {European Journal of Operational Research},
keywords = {Matching rates,Multi-type customers and servers,Parallel service systems,Queueing,Resource pooling},
pages = {259--277},
title = {{Stochastics and Statistics Design heuristic for parallel many server systems R}},
url = {https://doi.org/10.1016/j.ejor.2018.08.042},
volume = {273},
year = {2018}
}
@phdthesis{Kakade2003,
author = {Kakade, Sham Machandranath},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kakade - 2003 - On the Sample Complexity of Reinforcement Learning.pdf:pdf},
pages = {133},
school = {University College London},
title = {{On the Sample Complexity of Reinforcement Learning}},
year = {2003}
}
@book{Sennott1999,
abstract = {A path-breaking account of Markov decision processes-theory and computation. This book's clear presentation of theory, numerous chapter-end problems, and development of a unified method for the computation of optimal policies in both discrete and continuous time make it an excellent course text for graduate students and advanced undergraduates. Its comprehensive coverage of important recent advances in stochastic dynamic programming makes it a valuable working resource for operations research professionals, management scientists, engineers, and others. Stochastic Dynamic Programming and the Co. Optimization criteria -- Finite horizon optimization -- Infinite horizon discounted cost optimization -- An inventory model -- Average cost optimization for finite state spaces -- Average cost optimization theory for countable state spaces -- Computation of average cost optimal policies for infinite state spaces -- Optimization under actions at selected epochs -- Average cost optimization of continuous time processes -- Appendices -- Bibliography -- Index.},
author = {Sennott, Linn I.},
isbn = {9780470317037},
pages = {328},
publisher = {Wiley},
title = {{Stochastic dynamic programming and the control of queueing systems}},
year = {1999}
}
@article{Martins1996,
abstract = {This paper provides a rigorous proof of the connection between the optimal sequencing problem for a two-station, two-customer-class queueing network and the problem of control of a multidimensional diffusion process, obtained as a heavy traffic limit of the queueing problem. In particular, the diffusion problem, which is one of "singular control" of a Brownian motion, is used to develop policies which are shown to be asymptotically nearly optimal as the traffic intensity approaches one in the queueing network. The results are proved by a viscosity solution analysis of the related Hamilton-Jacobi-Bellman equations.},
author = {Martins, L. F. and Shreve, S. E. and Soner, H. M.},
doi = {10.1137/S0363012994265882},
issn = {03630129},
journal = {SIAM Journal on Control and Optimization},
keywords = {Brownian networks,Heavy traffic,Queueing,Stochastic control,Viscosity solutions},
number = {6},
pages = {2133--2171},
publisher = {Society for Industrial and Applied Mathematics Publications},
title = {{Heavy traffic convergence of a controlled, multiclass queueing system}},
volume = {34},
year = {1996}
}
@inproceedings{Ramirez-Hernandez2007,
author = {Ramirez-Hernandez, Jose A. and Fernandez, Emmanuel},
booktitle = {2007 IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning},
doi = {10.1109/ADPRL.2007.368189},
isbn = {1-4244-0706-0},
pages = {201--208},
publisher = {IEEE},
title = {{An Approximate Dynamic Programming Approach for Job Releasing and Sequencing in a Reentrant Manufacturing Line}},
url = {https://ieeexplore.ieee.org/document/4220834/},
year = {2007}
}
@article{Kumar1993a,
author = {Kumar, P. R.},
doi = {10.1007/BF01158930},
issn = {0257-0130},
journal = {Queueing Systems},
month = {mar},
number = {1-3},
pages = {87--110},
publisher = {Kluwer Academic Publishers},
title = {{Re-entrant lines}},
url = {http://link.springer.com/10.1007/BF01158930},
volume = {13},
year = {1993}
}
@inproceedings{Schulman2015,
abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
archivePrefix = {arXiv},
arxivId = {1502.05477},
author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
booktitle = {Proceeding ICML'15},
eprint = {1502.05477},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schulman et al. - 2015 - Trust Region Policy Optimization.pdf:pdf},
pages = {1889--1897},
title = {{Trust Region Policy Optimization}},
url = {http://arxiv.org/abs/1502.05477},
year = {2015}
}
@inproceedings{Kakade2002,
author = {Kakade, Sham and Langford, John},
booktitle = {Proceedings of the Nineteenth International Conference on Machine Learning},
isbn = {1558608737},
pages = {267--274},
publisher = {Morgan Kaufmann Publishers},
title = {{Approximately Optimal Approximate Reinforcement Learning}},
year = {2002}
}
@article{Harrison1990,
address = {New York, NY},
author = {Harrison, J. Michael and Wein, Lawrence M.},
doi = {10.1007/978-1-4684-0302-2},
isbn = {978-1-4684-0304-6},
journal = {Operations Research},
number = {6},
pages = {1052--1064},
publisher = {Springer US},
series = {Graduate Texts in Mathematics},
title = {{Scheduling Networks of Queues: Heavy Traffic Analysis of a Two-Station Closed Network}},
url = {http://link.springer.com/10.1007/978-1-4684-0302-2},
volume = {38},
year = {1990}
}
@inproceedings{Algorta2017,
abstract = {The game of Tetris is an important benchmark for research in artificial intelligence and machine learning. This paper provides a historical account of the algorithmic developments in Tetris and discusses open challenges. Hand-crafted controllers, genetic algorithms, and reinforcement learning have all contributed to good solutions. However, existing solutions fall far short of what can be achieved by expert players playing without time pressure. Further study of the game has the potential to contribute to important areas of research, including feature discovery , autonomous learning of action hierarchies, and sample-efficient reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1905.01652v2},
author = {Algorta, Simon and Simsek, Ozgur},
booktitle = {34th International Conference on Machine Learning},
eprint = {1905.01652v2},
isbn = {1905.01652v2},
title = {{The Game of Tetris in Machine Learning}},
url = {https://arxiv.org/pdf/1905.01652.pdf},
year = {2017}
}
@article{Propp1996,
author = {Propp, James Gary and Wilson, David Bruce},
doi = {10.1002/(SICI)1098-2418(199608/09)9:1/2<223::AID-RSA14>3.0.CO;2-O},
issn = {1042-9832},
journal = {Random Structures and Algorithms},
month = {aug},
number = {1-2},
pages = {223--252},
title = {{Exact sampling with coupled Markov chains and applications to statistical mechanics}},
url = {http://doi.wiley.com/10.1002/{\%}28SICI{\%}291098-2418{\%}28199608/09{\%}299{\%}3A1/2{\%}3C223{\%}3A{\%}3AAID-RSA14{\%}3E3.0.CO{\%}3B2-O},
volume = {9},
year = {1996}
}
@techreport{J2016,
abstract = {A recent goal in the Reinforcement Learning (RL) framework is to choose a sequence of actions or a policy to maximize the reward collected or minimize the regret incurred in a finite time horizon. For several RL problems in operation research and optimal control, the optimal policy of the underlying Markov Decision Process (MDP) is characterized by a known structure. The current state of the art algorithms do not utilize this known structure of the optimal policy while minimizing regret. In this work, we develop new RL algorithms that exploit the structure of the optimal policy to minimize regret. Numerical experiments on MDPs with structured optimal policies show that our algorithms have better performance, are easy to implement, have a smaller run-time and require less number of random number generations.},
archivePrefix = {arXiv},
arxivId = {1608.04929v1},
author = {J, Prabuchandran K and Bodas, Tejas and Tulabandhula, Theja},
eprint = {1608.04929v1},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/J, Bodas, Tulabandhula - 2016 - Reinforcement Learning algorithms for regret minimization in structured Markov Decision Processes.pdf:pdf},
title = {{Reinforcement Learning algorithms for regret minimization in structured Markov Decision Processes}},
url = {https://arxiv.org/pdf/1608.04929.pdf},
year = {2016}
}
@article{Machado2017,
abstract = {The Arcade Learning Environment (ALE) is an evaluation platform that poses the challenge of building AI agents with general competency across dozens of Atari 2600 games. It supports a variety of different problem settings and it has been receiving increasing attention from the scientific community, leading to some high-profile success stories such as the much publicized Deep Q-Networks (DQN). In this article we take a big picture look at how the ALE is being used by the research community. We show how diverse the evaluation methodologies in the ALE have become with time, and highlight some key concerns when evaluating agents in the ALE. We use this discussion to present some methodological best practices and provide new benchmark results using these best practices. To further the progress in the field, we introduce a new version of the ALE that supports multiple game modes and provides a form of stochasticity we call sticky actions. We conclude this big picture look by revisiting challenges posed when the ALE was introduced, summarizing the state-of-the-art in various problems and highlighting problems that remain open.},
archivePrefix = {arXiv},
arxivId = {1709.06009},
author = {Machado, Marlos C. and Bellemare, Marc G. and Talvitie, Erik and Veness, Joel and Hausknecht, Matthew and Bowling, Michael},
eprint = {1709.06009},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Machado et al. - 2017 - Revisiting the Arcade Learning Environment Evaluation Protocols and Open Problems for General Agents.pdf:pdf},
month = {sep},
title = {{Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents}},
url = {http://arxiv.org/abs/1709.06009},
year = {2017}
}
@article{Marbach2001,
abstract = {This paper proposes a simulation-based algorithm for optimizing the average reward in a finite-state Markov reward process that depends on a set of parameters. As a special case, the method applies to Markov decision processes where optimization takes place within a parametrized set of policies. The algorithm relies on the regenerative structure of finite-state Markov processes, involves the simulation of a single sample path, and can be implemented online. A convergence result (with probability 1) is provided View full abstract},
author = {Marbach, Peter and Tsitsiklis, John N.},
doi = {10.1109/9.905687},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Marbach, Tsitsiklis - 2001 - Simulation-based optimization of Markov reward processes.pdf:pdf},
issn = {00189286},
journal = {IEEE Transactions on Automatic Control},
keywords = {Markov reward processes,Simulation-based optimization,Stochastic approximation},
number = {2},
pages = {191--209},
title = {{Simulation-based optimization of Markov reward processes}},
volume = {46},
year = {2001}
}
@inproceedings{Wu2017,
author = {Wu, Yuhuai and Mansimov, Elman and Liao, Shun and Grosse, Roger and Ba, Jimmy},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {5285--5294},
title = {{Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation}},
url = {https://dl.acm.org/citation.cfm?id=3295280},
year = {2017}
}
@misc{Coady2017,
author = {Coady, Patrick},
title = {{AI Gym Workout}},
url = {https://learningai.io/projects/2017/07/28/ai-gym-workout.html},
urldate = {2019-02-23},
year = {2017}
}
@inproceedings{Henderson1997,
address = {New York, New York, USA},
author = {Henderson, Shane G. and Meyn, Sean P.},
booktitle = {Proceedings of the 29th conference on Winter simulation  - WSC '97},
doi = {10.1145/268437.268482},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Henderson, Meyn - 1997 - Efficient simulation of multiclass queueing networks.pdf:pdf},
isbn = {078034278X},
pages = {216--223},
publisher = {ACM Press},
title = {{Efficient simulation of multiclass queueing networks}},
url = {http://portal.acm.org/citation.cfm?doid=268437.268482},
year = {1997}
}
@article{Haarnoja2018,
abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
archivePrefix = {arXiv},
arxivId = {1801.01290},
author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
eprint = {1801.01290},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.pdf:pdf},
month = {jan},
title = {{Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}},
url = {http://arxiv.org/abs/1801.01290},
year = {2018}
}
@article{Silver2016,
abstract = {A computer Go program based on deep neural networks defeats a human professional player to achieve one of the grand challenges of artificial intelligence.},
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Silver et al. - 2016 - Mastering the game of Go with deep neural networks and tree search.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
keywords = {Computational science,Computer science,Reward},
month = {jan},
number = {7587},
pages = {484--489},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://www.nature.com/articles/nature16961},
volume = {529},
year = {2016}
}
@techreport{Stampa,
abstract = {In this paper we design and evaluate a Deep-Reinforcement Learning agent that optimizes routing. Our agent adapts automatically to current traffic conditions and proposes tailored configurations that attempt to minimize the network delay. Experiments show very promising performance. Moreover, this approach provides important operational advantages with respect to traditional optimization algorithms.},
archivePrefix = {arXiv},
arxivId = {1709.07080v1},
author = {Stampa, Giorgio and Arias, Marta and S{\'{a}}nchez-Charles, David and Munt{\'{e}}s-Mulero, Victor and Cabellos, Albert},
eprint = {1709.07080v1},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stampa et al. - Unknown - A Deep-Reinforcement Learning Approach for Software-Defined Networking Routing Optimization.pdf:pdf},
isbn = {1581137354},
keywords = {Deep Reinforcement Learning,Knowledge-Defined Networking,Routing optimization,SDN,Traffic Engineering},
title = {{A Deep-Reinforcement Learning Approach for Software-Defined Networking Routing Optimization}},
url = {http://knowledgedefinednetworking.org.}
}
@book{Kemeny1976,
abstract = {This textbook provides a systematic treatment of denumerable Markov chains, covering both the foundations of the subject and some in topics in potential theory and boundary theory. It is a discussion of relations among what might be called the descriptive quantities associated with Markov chains-probabilities of events and means of random variables that give insight into the behavior of the chains. The approach, by means of infinite matrices, simplifies the notation, shortens statements and proofs of theorems, and often suggests new results. This second edition includes the new chapter, Introduction to Random Fields, written by David Griffeath. 1: Prerequisites from Analysis -- 1. Denumerable Matrices -- 2. Measure Theory -- 3. Measurable Functions and Lebesgue Integration -- 4. Integration Theorems -- 5. Limit Theorems for Matrices -- 6. Some General Theorems from Analysis -- 2: Stochastic Processes -- 1. Sequence Spaces -- 2. Denumerable Stochastic Processes -- 3. Borel Fields in Stochastic Processes -- 4. Statements of Probability Zero or One -- 5. Conditional Probabilities -- 6. Random Variables and Means -- 7. Means Conditional on Statements -- 8. Problems -- 3: Martingales -- 1. Means Conditional on Partitions and Functions -- 2. Properties of Martingales -- 3. A First Martingale Systems Theorem -- 4. Martingale Convergence and a Second Systems Theorem -- 5. Examples of Convergent Martingales -- 6. Law of Large Numbers -- 7. Problems -- 4: Properties of Markov Chains -- 1. Markov Chains -- 2. Examples of Markov Chains -- 3. Applications of Martingale Ideas -- 4. Strong Markov Property -- 5. Systems Theorems for Markov Chains -- 6. Applications of Systems Theorems -- 7. Classification of States -- 8. Problems -- 5: Transient Chains -- 1. Properties of Transient Chains -- 2. Superregular Functions -- 3. Absorbing Chains -- 4. Finite Drunkard's Walk -- 5. Infinite Drunkard's Walk -- 6. A Zero-One Law for Sums of Independent Random Variables -- 7. Sums of Independent Random Variables on the Line -- 8. Examples of Sums of Independent Random Variables -- 9. Ladder Process for Sums of Independent Random Variables -- 10. The Basic Example -- 11. Problems -- 6: Recurrent Chains -- 1. Mean Ergodic Theorem for Markov Chains -- 2. Duality -- 3. Cyclicity -- 4. Sums of Independent Random Variables -- 5. Convergence Theorem for Noncyclic Chains -- 6. Mean First Passage Time Matrix -- 7. Examples of the Mean First Passage Time Matrix -- 8. Reverse Markov Chains -- 9. Problems -- 7: Introduction to Potential Theory -- 1. Brownian Motion -- 2. Potential Theory -- 3. Equivalence of Brownian Motion and Potential Theory -- 4. Brownian Motion and Potential Theory in n Dimensions -- 5. Potential Theory for Denumerable Markov Chains -- 6. Brownian Motion as a Limit of the Symmetric Random Walk -- 7. Symmetric Random Walk in n Dimensions -- 8: Transient Potential Theory -- 1. Potentials -- 2. The h-Process and Some Applications -- 3. Equilibrium Sets and Capacities -- 4. Potential Principles -- 5. Energy -- 6. The Basic Example -- 7. An Unbounded Potential -- 8. Applications of Potential-Theoretic Methods -- 9. General Denumerable Stochastic Processes -- 10. Problems -- 9: Recurrent Potential Theory -- 1. Potentials -- 2. Normal Chains -- 3. Ergodic Chains -- 4. Classes of Ergodic Chains -- 5. Strong Ergodic Chains -- 6. The Basic Example -- 7. Further Examples -- 8. The Operator K -- 9. Potential Principles -- 10. A Model for Potential Theory -- 11. A Nonnormal Chain and Other Examples -- 12. Two-Dimensional Symmetric Random Walk -- 13. Problems -- 10: Transient Boundary Theory -- 1. Motivation for Martin Boundary Theory -- 2. Extended Chains -- 3. Martin Exit Boundary -- 4. Convergence to the Boundary -- 5. Poisson-Martin Representation Theorem -- 6. Extreme Points of the Boundary -- 7. Uniqueness of the Representation -- 8. Analog of Fatou's Theorem -- 9. Fine Boundary Functions -- 10. Martin Entrance Boundary -- 11. Application to Extended Chains -- 12. Proof of Theorem 10.9 -- 13. Examples -- 14. Problems -- 11: Recurrent Boundary Theory -- 1. Entrance Boundary for Recurrent Chains -- 2. Measures on the Entrance Boundary -- 3. Harmonic Measure for Normal Chains -- 4. Continuous and T-Continuous Functions -- 5. Normal Chains and Convergence to the Boundary -- 6. Representation Theorem -- 7. Sums of Independent Random Variables -- 8. Examples -- 9. Problems -- 12: Introduction to Random Fields -- 1 Markov Fields -- 2. Finite Gibbs Fields -- 3. Equivalence of Finite Markov and Neighbor Gibbs Fields -- 4. Markov Fields and Neighbor Gibbs Fields: the Infinite Case -- 5. Homogeneous Markov Fields on the Integers -- 6. Examples of Phase Multiplicity in Higher Dimensions -- 7. Problems -- Notes -- Additional Notes -- References -- Additional References -- Index of Notation.},
author = {Kemeny, John G. and Snell, J. Laurie. and Knapp, Anthony W.},
isbn = {9781468494570},
pages = {XII, 484},
publisher = {Springer New York},
title = {{Denumerable Markov Chains}},
year = {1976}
}
@techreport{Greensmith2004,
abstract = {Policy gradient methods for reinforcement learning avoid some of the undesirable properties of the value function approaches, such as policy degradation (Baxter and Bartlett, 2001). However, the variance of the performance gradient estimates obtained from the simulation is sometimes excessive. In this paper, we consider variance reduction methods that were developed for Monte Carlo estimates of integrals. We study two commonly used policy gradient techniques, the baseline and actor-critic methods, from this perspective. Both can be interpreted as additive control variate variance reduction methods. We consider the expected average reward performance measure, and we focus on the GPOMDP algorithm for estimating performance gradients in partially observable Markov decision processes controlled by stochastic reactive policies. We give bounds for the estimation error of the gradient estimates for both baseline and actor-critic algorithms, in terms of the sample size and mixing properties of the controlled system. For the baseline technique, we compute the optimal baseline, and show that the popular approach of using the average reward to define the baseline can be suboptimal. For actor-critic algorithms, we show that using the true value function as the critic can be suboptimal. We also discuss algorithms for estimating the optimal baseline and approximate value function.},
author = {Greensmith, Evan and Bartlett, Peter L and Edu, Bartlett@stat Berkeley and Baxter, Jonathan},
booktitle = {Journal of Machine Learning Research},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Greensmith et al. - 2004 - Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning.pdf:pdf},
keywords = {GPOMDP,actor-critic,baseline,policy gradient,reinforcement learning},
pages = {1471--1530},
title = {{Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning}},
url = {http://jmlr.csail.mit.edu/papers/volume5/greensmith04a/greensmith04a.pdf},
volume = {5},
year = {2004}
}
@inproceedings{Jaakkola1994,
author = {Jaakkola, Tommi and {P. Singh}, Satinder and Jordan, Michael I.},
booktitle = {Proceedings of the 7th International Conference on Neural Information Processing Systems},
pages = {345--352},
title = {{Reinforcement learning algorithm for partially observable Markov decision problems}},
url = {https://dl.acm.org/citation.cfm?id=2998730},
year = {1994}
}
@article{Fill1991,
author = {Fill, James Allen},
doi = {10.1214/aoap/1177005981},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fill - 1991 - Eigenvalue Bounds on Convergence to Stationarity for Nonreversible Markov Chains, with an Application to the Exclusion Pro.pdf:pdf},
issn = {1050-5164},
journal = {The Annals of Applied Probability},
keywords = {Cheege's inequality,Markov chains,Poincare inequality,Poisson blockers,chi-square distance,exclusion process,interacting particle systems,rapid mixing,rates of convergence,reversibility,variation distance},
month = {feb},
number = {1},
pages = {62--87},
publisher = {Institute of Mathematical Statistics},
title = {{Eigenvalue Bounds on Convergence to Stationarity for Nonreversible Markov Chains, with an Application to the Exclusion Process}},
url = {http://projecteuclid.org/euclid.aoap/1177005981},
volume = {1},
year = {1991}
}
@article{Rudolf2011,
abstract = {We prove explicit, i.e. non-asymptotic, error bounds for Markov chain Monte Carlo methods. The problem is to compute the expectation of a function f with respect to a measure {\{}$\backslash$pi{\}}. Different convergence properties of Markov chains imply different error bounds. For uniformly ergodic and reversible Markov chains we prove a lower and an upper error bound with respect to the L2 -norm of f . If there exists an L2 -spectral gap, which is a weaker convergence property than uniform ergodicity, then we show an upper error bound with respect to the Lp -norm of f for p {\textgreater} 2. Usually a burn-in period is an efficient way to tune the algorithm. We provide and justify a recipe how to choose the burn-in period. The error bounds are applied to the problem of the integration with respect to a possibly unnormalized density. More precise, we consider the integration with respect to log-concave densities and the integration over convex bodies. By the use of the Metropolis algorithm based on a ball walk and the hit-and-run algorithm it is shown that both problems are polynomial tractable.},
archivePrefix = {arXiv},
arxivId = {1108.3201},
author = {Rudolf, Daniel},
eprint = {1108.3201},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rudolf - 2011 - Explicit error bounds for Markov chain Monte Carlo.pdf:pdf},
month = {aug},
title = {{Explicit error bounds for Markov chain Monte Carlo}},
url = {http://arxiv.org/abs/1108.3201},
year = {2011}
}
@article{Abbasi_Yadkori2014,
abstract = {To reveal the antiangiogenic capability of cancer chemotherapy, we developed an alternative antiangiogenic schedule for administration of cyclophosphamide. We show here that this antiangiogenic schedule avoided drug resistance and eradicated Lewis lung carcinoma and L1210 leukemia, an outcome not possible with the conventional schedule. When Lewis lung carcinoma and EMT-6 breast cancer were made drug resistant before therapy, the antiangiogenic schedule suppressed tumor growth 3-fold more effectively than the conventional schedule. When another angiogenesis inhibitor, TNP-470, was added to the antiangiogenic schedule of cyclophosphamide, drug-resistant Lewis lung carcinomas were eradicated. Each dose of the antiangiogenic schedule of cyclophosphamide induced the apoptosis of endothelial cells within tumors, and endothelial cell apoptosis preceded the apoptosis of drug-resistant tumor cells. This antiangiogenic effect was more pronounced in p53-null mice in which the apoptosis of p53-null endothelial cells induced by cyclophosphamide was so vigorous that drug-resistant tumors comprising 4.5{\%} of body weight were eradicated. Thus, by using a dosing schedule of cyclophosphamide that provided more sustained apoptosis of endothelial cells within the vascular bed of a tumor, we show that a chemotherapeutic agent can more effectively control tumor growth in mice, regardless of whether the tumor cells are drug resistant.},
author = {Browder, Timothy and Butterfield, Catherine E. and Kr{\"{a}}ling, Birgit M. and Shi, Bin and Marshall, Blair and O'Reilly, Michael S. and Folkman, Judah},
issn = {00085472},
journal = {Cancer Research},
number = {7},
pages = {1878--1886},
title = {{Antiangiogenic scheduling of chemotherapy improves efficacy against experimental drug-resistant cancer}},
url = {https://pdf.sciencedirectassets.com/280252/1-s2.0-S1876735414X00024/1-s2.0-S1876735414000233/main.pdf?x-amz-security-token=AgoJb3JpZ2luX2VjEFkaCXVzLWVhc3QtMSJGMEQCIHqKzXvdF9ZYkh0GtA6iGtMnTQfPbmNOJg3mUiyTOUHsAiByprtXypGwMqNPy9VtGiSKsZ8bmJxpuJ{\%}2BdASqirxGsbC},
volume = {60},
year = {2000}
}
@inproceedings{Kimura1998,
abstract = {We present an analysis of actor/critic algorithms , in which the actor updates its policy using eligibility traces of the policy parameters. Most of the theoretical results for eligibility traces have been for only critic's value iteration algorithms. This paper investigates what the actor's eligibility trace does. The results show that the algorithm is an extension of Williams' REINFORCE algorithms for innite horizon reinforcement tasks, and then the critic provides an appropriate reinforcement baseline for the actor. Thanks to the actor's eligibility trace, the actor improves its policy by using a gradient of actual return, not by using a gradient of the estimated return in the critic. It enables the agent to learn a fairly good policy under the condition that the approximated value function in the critic is hopelessly inaccurate for conventional actor/critic algorithms. Also, if an accurate value function is estimated by the critic, the actor's learning is dramatically accelerated in our test cases. The behavior of the algorithm is demonstrated through simulations of a linear quadratic control problem and a pole balancing problem.},
author = {Kimura, Hajime and Kobayashi, Shigenobu},
booktitle = {ICML},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kimura, Kobayashi - 1998 - An Analysis of ActorCritic Algorithms using Eligibility Traces Reinforcement Learning with Imperfect Value Fu.pdf:pdf},
pages = {278--286},
title = {{An Analysis of Actor/Critic Algorithms using Eligibility Traces: Reinforcement Learning with Imperfect Value Functions}},
year = {1998}
}
@inproceedings{Krishnasamy2019,
abstract = {We consider learning-based variants of the c$\mu$ rule for scheduling in single and parallel server settings of multiclass queueing systems.In the single server setting, the c$\mu$ rule is known to minimize the expected holding-cost (weighted queue-lengths summed over classes and a fixed time horizon). We focus on the problem where the service rates $\mu$ are unknown with the holding-cost regret (regret against the c$\mu$ rule with known $\mu$) as our objective. We show that the greedy algorithm that uses empirically learned service rates results in a constant holding-cost regret (the regret is independent of the time horizon). This free exploration can be explained in the single server setting by the fact that any work-conserving policy obtains the same number of samples in a busy cycle.In the parallel server setting, we show that the c$\mu$ rule may result in unstable queues, even for arrival rates within the capacity region. We then present sufficient conditions for geometric ergodicity under the c$\mu$ rule. Using these results, we propose an almost greedy algorithm that explores only when the number of samples falls below a threshold. We show that this algorithm delivers constant holding-cost regret because a free exploration condition is eventually satisfied.},
archivePrefix = {arXiv},
arxivId = {1802.06723},
author = {Krishnasamy, Subhashini and Arapostathis, Ari and Johari, Ramesh and Shakkottai, Sanjay},
booktitle = {2018 56th Annual Allerton Conference on Communication, Control, and Computing, Allerton 2018},
doi = {10.1109/ALLERTON.2018.8636001},
eprint = {1802.06723},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Krishnasamy et al. - 2019 - On Learning the c$\mu$ Rule in Single and Parallel Server Networks.pdf:pdf},
isbn = {9781538665961},
keywords = {C$\mu$ rule,learning,queueing systems,stability},
month = {feb},
pages = {153--154},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{On Learning the c$\mu$ Rule in Single and Parallel Server Networks}},
year = {2019}
}
@article{Veatch2015,
abstract = {This paper uses approximate linear programming (ALP) to compute average cost bounds for queueing network control problems. Like most approximate dynamic programming (ADP) methods, ALP approximates the differential cost by a linear form. New types of approximating functions are identified that offer more accuracy than previous ALP studies or other performance bound methods. The structure of the infinite constraint set is exploited to reduce it to a more manageable set. When needed, constraint sampling and truncation methods are also developed. Numerical experiments show that the LPs using quadratic approximating functions can be easily solved on examples with up to 17 buffers. Using additional functions reduced the error to 1–5{\%} at the cost of larger LPs. These ALPs were solved for systems with up to 6–11 buffers, depending on the functions used. The method computes bounds much faster than value iteration. It also gives some insights into policies. The ALPs do not scale to very large problems, but they offer more accurate bounds than other methods and the simplicity of just solving an LP.},
author = {Veatch, Michael H.},
doi = {10.1016/j.cor.2015.04.014},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Veatch - 2015 - Approximate linear programming for networks Average cost bounds(2).pdf:pdf},
issn = {03050548},
journal = {Computers {\&} Operations Research},
pages = {32--45},
title = {{Approximate linear programming for networks: Average cost bounds}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0305054815000945 http://linkinghub.elsevier.com/retrieve/pii/S0305054815000945},
volume = {63},
year = {2015}
}
@article{Meketon1982,
author = {Meketon, Marc S and Heidelberger, Philip},
journal = {Management Science},
number = {2},
pages = {173--182},
title = {{A Renewal Theoretic Approach to Bias Reduction in Regenerative Simulations}},
url = {https://search-proquest-com.proxy.library.cornell.edu/docview/213220992?pq-origsite=summon},
volume = {28},
year = {1982}
}
@inproceedings{Wu2018,
abstract = {Policy gradient methods have enjoyed great success in deep reinforcement learning but suffer from high variance of gradient estimates. The high variance problem is particularly exasperated in problems with long horizons or high-dimensional action spaces. To mitigate this issue, we derive a bias-free action-dependent baseline for variance reduction which fully exploits the structural form of the stochastic policy itself and does not make any additional assumptions about the MDP. We demonstrate and quantify the benefit of the action-dependent baseline through both theoretical analysis as well as numerical results, including an analysis of the suboptimality of the optimal state-dependent baseline. The result is a computationally efficient policy gradient algorithm, which scales to high-dimensional control problems, as demonstrated by a synthetic 2000-dimensional target matching task. Our experimental results indicate that action-dependent baselines allow for faster learning on standard reinforcement learning benchmarks and high-dimensional hand manipulation and synthetic tasks. Finally, we show that the general idea of including additional information in baselines for improved variance reduction can be extended to partially observed and multi-agent tasks.},
archivePrefix = {arXiv},
arxivId = {1803.07246},
author = {Wu, Cathy and Rajeswaran, Aravind and Duan, Yan and Kumar, Vikash and Bayen, Alexandre M and Kakade, Sham and Mordatch, Igor and Abbeel, Pieter},
booktitle = {. International Conference on Learning Representations},
eprint = {1803.07246},
month = {mar},
title = {{Variance Reduction for Policy Gradient with Action-Dependent Factorized Baselines}},
url = {http://arxiv.org/abs/1803.07246},
year = {2018}
}
@article{Wein1990,
author = {Wein, Lawrence M.},
doi = {10.1287/moor.15.2.215},
issn = {0364-765X},
journal = {Mathematics of Operations Research},
keywords = {Brownian approximations,heavy traffic analysis,networks of queues,singular control},
month = {may},
number = {2},
pages = {215--242},
publisher = {INFORMS},
title = {{Optimal Control of a Two-Station Brownian Network}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/moor.15.2.215},
volume = {15},
year = {1990}
}
@inproceedings{Grathwohl2018,
abstract = {Gradient-based optimization is the foundation of deep learning and reinforcement learning. Even when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy. We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables. Our method uses gradients of a neural network trained jointly with model parameters or policies, and is applicable in both discrete and continuous settings. We demonstrate this framework for training discrete latent-variable models. We also give an unbiased, action-conditional extension of the advantage actor-critic reinforcement learning algorithm.},
archivePrefix = {arXiv},
arxivId = {1711.00123},
author = {Grathwohl, Will and Choi, Dami and Wu, Yuhuai and Roeder, Geoffrey and Duvenaud, David},
booktitle = {International Conference on Learning Representations (ICLR)},
eprint = {1711.00123},
month = {oct},
title = {{Backpropagation through the Void: Optimizing control variates for black-box gradient estimation}},
url = {http://arxiv.org/abs/1711.00123},
year = {2018}
}
@article{Paschalidis2004,
author = {Paschalidis, I.C. and Su, C. and Caramanis, M.C.},
doi = {10.1109/TAC.2004.835389},
issn = {0018-9286},
journal = {IEEE Transactions on Automatic Control},
number = {10},
pages = {1709--1722},
title = {{Target-Pursuing Scheduling and Routing Policies for Multiclass Queueing Networks}},
url = {http://ieeexplore.ieee.org/document/1341564/},
volume = {49},
year = {2004}
}
@book{Csiszar2011,
abstract = {2nd ed. Fully updated and revised edition of Csiszár and Körner's classic book on information theory. Cover; Information Theory; Title; Copyright; Contents; Preface to the first edition; Preface to the second edition; Basic notation and conventions; Preliminaries on random variables and probability distributions; Introduction; Intuitive background; Informal description of the basic mathematical model; Measuring information; Multi-terminal systems; Part I Information measures in simple coding problems; 1 Source coding and hypothesis testing; information measures; Discussion; Problems; Postulational characterizations of entropy (Problems 1.11-1.14); Story of the results. 2 Types and typical sequencesDiscussion; Problems; Story of the results; 3 Formal properties of Shannon's information measures; Problems; Properties of informational divergence (Problems 3.17-3.20); Structural results on entropy (Problems 3.21-3.22); Story of the results; 4 Non-block source coding; Problems; General noiseless channels (Problems 4.20-4.22); Universal variable-length codes (Problems 4.23-4.26); Story of the results; 5 Blowing up lemma: a combinatorial digression; Problems; Story of the results; Part II Two-terminal systems; 6 The noisy channel coding problem; Discussion. ProblemsComparison of channels (Problems 6.16-6.18); Zero-error capacity and graphs (Problems 6.23-6.25); Story of the results; 7 Rate-distortion trade-off in source coding and the source-channel transmission problem; Discussion; Problems; Story of the results; 8 Computation of channel capacity and?-distortion rates; Problems; Story of the results; 9 A covering lemma and the error exponent in source coding; Problems; Graph entropy and convex corners; Story of the results; 10 A packing lemma and the error exponent in channel coding; Discussion; Problems; Compound DMCs (Problems 10.12-10.14). Reliability at R = 0 (Problems 10.20-10.23)Story of the results; 11 The compound channel revisited: zero-error information theory and extremal combinatorics; Discussion; Problems; Story of the results; 12 Arbitrarily varying channels; Discussion; Problems; Story of the results; Part III Multi-terminal systems; 13 Separate coding of correlated sources; Discussion; Problems; Story of the results; 14 Multiple-access channels; Discussion; Problems; Reduction of channel network problems (Problems 14.22-14.24); Story of the results; 15 Entropy and image size characterization; Discussion; Problems. Image size of arbitrary sets (Problems 15.4-15.5)More-than-three-component sources (Problems 15.16-15.21); Story of the results; 16 Source and channel networks; Discussion; Problems; Broadcast channels (Problems 16.8-16.12); Source networks with three inputs and one helper (Problems 16.13-16.18); Source networks with two helpers; General fidelity criteria (Problems 16.22-16.24); Common information (Problems 16.27-16.30); Miscellaneous source networks (Problems 16.31-16.33); Story of the results; 17 Information-theoretic security; 17.1 Basic concepts and tools.},
author = {Csiszár, Imre and Körner, János.},
isbn = {9780521196819},
pages = {499},
publisher = {Cambridge University Press},
title = {{Information theory : coding theorems for discrete memoryless systems}},
url = {https://www.cambridge.org/gb/academic/subjects/engineering/communications-and-signal-processing/information-theory-coding-theorems-discrete-memoryless-systems-2nd-edition?format=HB{\&}isbn=9780521196819},
year = {2011}
}
@article{Baxter2001,
abstract = {Gradient-based approaches to direct policy search in reinforcement learning have received much recent attention as a means to solve problems of partial observability and to avoid some of the problems associated with policy degradation in value-function methods. In this paper we introduce {\`{E}}{\c{C}}{\AA}{\AA}{\`{E}}, a simulation-based algorithm for generating a biased estimate of the gradient of the average reward in Partially Observable Markov Decision Processes ({\`{E}}{\c{C}}{\AA}{\AA}{\`{E}}s) controlled by parameterized stochastic policies. A similar algorithm was proposed by Kimura, Yamamura, and Kobayashi (1995). The algorithm's chief advantages are that it requires storage of only twice the number of policy parameters, uses one free parameter ¬ ¾ ¼ ½µ (which has a natural interpretation in terms of bias-variance trade-off), and requires no knowledge of the underlying state. We prove convergence of {\`{E}}{\c{C}}{\AA}{\AA}{\`{E}}, and show how the correct choice of the parameter ¬ is related to the mixing time of the controlled {\`{E}}{\c{C}}{\AA}{\AA}{\`{E}}. We briefly describe extensions of {\`{E}}{\c{C}}{\AA}{\AA}{\`{E}} to controlled Markov chains, continuous state, observation and control spaces, multiple-agents, higher-order derivatives, and a version for training stochastic policies with internal states. In a companion paper (Baxter, Bartlett, {\&} Weaver, 2001) we show how the gradient estimates generated by {\`{E}}{\c{C}}{\AA}{\AA}{\`{E}} can be used in both a traditional stochastic gradient algorithm and a conjugate-gradient procedure to find local optima of the average reward.},
author = {Baxter, Jonathan and Bartlett, Peter L},
journal = {Journal of Artificial Intelligence Research},
pages = {319--350},
title = {{Infinite-Horizon Policy-Gradient Estimation}},
volume = {15},
year = {2001}
}
@article{Dai1996,
abstract = {Reentrant lines can be used to model complex manufacturing systems such as wafer fabrication facilities. As the first step to the optimal or near-optimal scheduling of such lines, we investigate th...},
author = {Dai, J. G. and Weiss, G.},
doi = {10.1287/moor.21.1.115},
issn = {0364-765X},
journal = {Mathematics of Operations Research},
keywords = {Harris recurrence,fluid models,multiclass queueing networks,piecewise linear Lyapunoc functions,reentrant lines,scheduling policies,stability,unstable networks},
number = {1},
pages = {115--134},
publisher = {INFORMS},
title = {{Stability and Instability of Fluid Models for Reentrant Lines}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/moor.21.1.115},
volume = {21},
year = {1996}
}
@article{Nichol2018a,
abstract = {In this report, we present a new reinforcement learning (RL) benchmark based on the Sonic the Hedgehog (TM) video game franchise. This benchmark is intended to measure the performance of transfer learning and few-shot learning algorithms in the RL domain. We also present and evaluate some baseline algorithms on the new benchmark.},
archivePrefix = {arXiv},
arxivId = {1804.03720},
author = {Nichol, Alex and Pfau, Vicki and Hesse, Christopher and Klimov, Oleg and Schulman, John},
eprint = {1804.03720},
file = {:C$\backslash$:/Users/mg2289/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nichol et al. - 2018 - Gotta Learn Fast A New Benchmark for Generalization in RL.pdf:pdf},
month = {apr},
title = {{Gotta Learn Fast: A New Benchmark for Generalization in RL}},
url = {http://arxiv.org/abs/1804.03720},
year = {2018}
}
